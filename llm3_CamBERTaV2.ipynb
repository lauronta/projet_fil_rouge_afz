{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "# from transformers import AutoTokenizer\n",
    "from transformers import BertForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détermination du Path\n",
    "\n",
    "DATASET_PATH = Path(\"./data/text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des données textuelles\n",
    "\n",
    "VOC_SIZE = 1000\n",
    "\n",
    "def load_data(datapath, max_size=None):\n",
    "    texts_files = list(datapath.glob(\"*.txt\"))\n",
    "    texts = []  \n",
    "    for files in texts_files:\n",
    "        with open(files, \"r\", encoding='utf8') as files:\n",
    "            text = files.readlines()\n",
    "            texts += text\n",
    "    texts = list(set(texts))\n",
    "    \n",
    "    return texts\n",
    "\n",
    "texts = load_data(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deberta-v2 to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at almanach/camembertav2-base and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Dr', '##y', 'for', '##ages', 'Ric', '##e', 'be', '##an', '(', 'Vig', '##na', 'u', '##mb', '##ella', '##ta', '),', 'ha', '##y', 'Ric', '##e', 'be', '##an', '(', 'Vig', '##na', 'u', '##mb', '##ella', '##ta', '(', 'Th', '##un', '##b', '.)', 'Oh', '##wi', '&', 'Oh', '##ashi', '),', 'ha', '##y', '\\n', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle CamemBERTav2 et tokenisation du texte\n",
    "\n",
    "model_checkpoint = \"almanach/camembertav2-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = BertForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', max_length=100\n",
    "                   , truncation=True, padding='max_length')\n",
    "\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "print(inputs.tokens(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the [MASK] token with the mask array \n",
    "inputs.input_ids[mask_arr] = 103\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, idx):\n",
    "        self.encodings = encodings\n",
    "        self.idx = idx\n",
    "        self.encodings = {key: [val[i] for i in self.idx] for key, val in self.encodings.items()}\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "sample_idx = [i for i in range(len(inputs.input_ids))]\n",
    "\n",
    "shuffled_sample_idx = rd.sample(sample_idx, len(sample_idx))\n",
    "\n",
    "train_idx = shuffled_sample_idx[:int(0.70*len(shuffled_sample_idx))]\n",
    "val_idx = shuffled_sample_idx[int(0.70*len(shuffled_sample_idx)):int(0.85*len(shuffled_sample_idx))]\n",
    "test_idx = shuffled_sample_idx[int(0.85*len(shuffled_sample_idx)):]\n",
    "                                \n",
    "dataset_train = CustomDataset(inputs, train_idx)\n",
    "dataset_val = CustomDataset(inputs, val_idx)\n",
    "dataset_test = CustomDataset(inputs, test_idx)\n",
    "\n",
    "train_dataloaded = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "val_dataloaded = torch.utils.data.DataLoader(dataset_val, batch_size=16, shuffle=True)\n",
    "test_dataloaded = torch.utils.data.DataLoader(dataset_test, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MLM_model(nn.Module):\n",
    "#    def __init__(self, model):\n",
    "#        super(MLM_model, self).__init__()\n",
    "#        self.history = {\"epochs\":[], \"test\":[]}\n",
    "#        self.model = model\n",
    "    \n",
    "#    def parameters(self):\n",
    "#        return self.model.parameters()\n",
    "\n",
    "#    def forward(self, x, attention_mask, labels):\n",
    "#        return self.model(x, attention_mask, labels)\n",
    "    \n",
    "#    def train_log(self, train_batch_losses, val_batch_losses, train_loss, validation_loss):\n",
    "#        self.history[\"epochs\"].append({\"train_batch_losses\":train_batch_losses, \n",
    "#                                \"val_batch_losses\":val_batch_losses, \n",
    "#                                \"train_loss\":train_loss, \n",
    "#                                \"validation_loss\":validation_loss})\n",
    "    \n",
    "#    def test_log(self, test_batch_losses, test_loss):\n",
    "#        self.history[\"test\"].append({\"test_batch_losses\":test_batch_losses,\n",
    "#                                \"test_loss\":test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#model = MLM_model(model)\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(module, batch, batch_idx, optimizer):\n",
    "    module.train(True)\n",
    "    \n",
    "    inputs_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    outputs = module(inputs_ids, attention_mask, labels=labels)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    print(f\"\\n\\033[1;37mBatch loss {batch_idx+1} : {loss.item()}\")\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(module.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return module, loss\n",
    "\n",
    "def eval_step(module, batch, batch_idx, optimizer=None, training=True):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inputs_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        outputs = module(inputs_ids, attention_mask, labels=labels)\n",
    "    \n",
    "        loss = outputs.loss\n",
    "         \n",
    "        if training:\n",
    "            print(f\"\\n\\033[1;32mValidation Batch loss {batch_idx+1} : {loss.item()}\")\n",
    "            return module, loss\n",
    "        else:\n",
    "            print(f\"\\n\\033[1;32mTest Batch loss {batch_idx+1} : {loss.item()}\")\n",
    "            return module, loss, outputs, labels\n",
    "\n",
    "def train_loop(module, EPOCHS, train_dataset, val_dataset, optimizer, lr_scheduler=None):\n",
    "    for epoch in range(EPOCHS):\n",
    "        deb=time.time()\n",
    "        \n",
    "        module.train(True)\n",
    "        \n",
    "        train_batch_losses = []\n",
    "        for batch_idx in range(len(train_dataset)):\n",
    "            batch = next(iter(train_dataset))\n",
    "            module, loss = train_step(module, batch, batch_idx, optimizer)\n",
    "            train_batch_losses.append(loss.item())\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "          lr_scheduler.step()\n",
    "        train_loss = np.mean(train_batch_losses)\n",
    "\n",
    "        module.train(False)\n",
    "        val_batch_losses = []\n",
    "        for batch_idx in range(len(val_dataset)):\n",
    "            batch = next(iter(val_dataset))\n",
    "            module, loss = eval_step(module, batch, batch_idx)\n",
    "            val_batch_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "\n",
    "#        module.train_log(train_batch_losses, val_batch_losses, train_loss, val_loss)\n",
    "        print(f\"\\n\\033[1;33mEpoch {epoch+1} :\\n\\033[1;37mTraining Loss : {train_loss}\")\n",
    "        print(f\"\\033[1;32mValidation Loss : {val_loss}\")\n",
    "        print(f\"\\033[1;31mDurée epoch : {time.time()-deb} secondes\")\n",
    "    return module\n",
    "\n",
    "def evaluate(module, test_dataset):\n",
    "    module.train(False)\n",
    "    test_batch_losses = []\n",
    "    predictions = []\n",
    "    true_targets = []\n",
    "    for batch_idx in range(len(test_dataset)):\n",
    "        batch = next(iter(test_dataset))\n",
    "        module, loss, outputs, labels = eval_step(module, batch, batch_idx, training=False)\n",
    "\n",
    "        test_batch_losses.append(loss.item())\n",
    "        predictions.append(outputs)\n",
    "        true_targets.append(labels)\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "#    module.test_log(test_batch_losses, test_loss)\n",
    "    print(f\"\\nTest Loss : {test_loss}\")\n",
    "    return predictions, true_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateo\\AppData\\Local\\Temp\\ipykernel_16816\\2355751477.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;37mBatch loss 1 : 10.690932273864746\n",
      "\n",
      "\u001b[1;37mBatch loss 2 : 7.545878887176514\n",
      "\n",
      "\u001b[1;37mBatch loss 3 : 5.737115859985352\n",
      "\n",
      "\u001b[1;37mBatch loss 4 : 5.110807418823242\n",
      "\n",
      "\u001b[1;37mBatch loss 5 : 4.460752487182617\n",
      "\n",
      "\u001b[1;37mBatch loss 6 : 4.176612377166748\n",
      "\n",
      "\u001b[1;37mBatch loss 7 : 3.3132517337799072\n",
      "\n",
      "\u001b[1;37mBatch loss 8 : 2.7446494102478027\n",
      "\n",
      "\u001b[1;37mBatch loss 9 : 2.3033745288848877\n",
      "\n",
      "\u001b[1;37mBatch loss 10 : 2.031712055206299\n",
      "\n",
      "\u001b[1;37mBatch loss 11 : 1.575617790222168\n",
      "\n",
      "\u001b[1;37mBatch loss 12 : 1.3664805889129639\n",
      "\n",
      "\u001b[1;37mBatch loss 13 : 1.1571484804153442\n",
      "\n",
      "\u001b[1;37mBatch loss 14 : 0.8873598575592041\n",
      "\n",
      "\u001b[1;37mBatch loss 15 : 0.7502452731132507\n",
      "\n",
      "\u001b[1;37mBatch loss 16 : 0.592736005783081\n",
      "\n",
      "\u001b[1;37mBatch loss 17 : 0.538517415523529\n",
      "\n",
      "\u001b[1;37mBatch loss 18 : 0.604697585105896\n",
      "\n",
      "\u001b[1;37mBatch loss 19 : 0.3212076425552368\n",
      "\n",
      "\u001b[1;37mBatch loss 20 : 0.4435454308986664\n",
      "\n",
      "\u001b[1;37mBatch loss 21 : 0.25785577297210693\n",
      "\n",
      "\u001b[1;37mBatch loss 22 : 0.26058661937713623\n",
      "\n",
      "\u001b[1;37mBatch loss 23 : 0.2633827030658722\n",
      "\n",
      "\u001b[1;37mBatch loss 24 : 0.25170719623565674\n",
      "\n",
      "\u001b[1;37mBatch loss 25 : 0.3286091983318329\n",
      "\n",
      "\u001b[1;37mBatch loss 26 : 0.20134860277175903\n",
      "\n",
      "\u001b[1;37mBatch loss 27 : 0.24497640132904053\n",
      "\n",
      "\u001b[1;37mBatch loss 28 : 0.15108948945999146\n",
      "\n",
      "\u001b[1;37mBatch loss 29 : 0.2951587438583374\n",
      "\n",
      "\u001b[1;37mBatch loss 30 : 0.09687039256095886\n",
      "\n",
      "\u001b[1;37mBatch loss 31 : 0.19620636105537415\n",
      "\n",
      "\u001b[1;37mBatch loss 32 : 0.1827559620141983\n",
      "\n",
      "\u001b[1;37mBatch loss 33 : 0.1425502896308899\n",
      "\n",
      "\u001b[1;37mBatch loss 34 : 0.17157502472400665\n",
      "\n",
      "\u001b[1;37mBatch loss 35 : 0.16369017958641052\n",
      "\n",
      "\u001b[1;37mBatch loss 36 : 0.1411425620317459\n",
      "\n",
      "\u001b[1;37mBatch loss 37 : 0.10042010247707367\n",
      "\n",
      "\u001b[1;37mBatch loss 38 : 0.07633233815431595\n",
      "\n",
      "\u001b[1;37mBatch loss 39 : 0.11390000581741333\n",
      "\n",
      "\u001b[1;37mBatch loss 40 : 0.1693631410598755\n",
      "\n",
      "\u001b[1;37mBatch loss 41 : 0.08625783771276474\n",
      "\n",
      "\u001b[1;37mBatch loss 42 : 0.10707514733076096\n",
      "\n",
      "\u001b[1;37mBatch loss 43 : 0.1484133005142212\n",
      "\n",
      "\u001b[1;37mBatch loss 44 : 0.1665549874305725\n",
      "\n",
      "\u001b[1;37mBatch loss 45 : 0.17124688625335693\n",
      "\n",
      "\u001b[1;37mBatch loss 46 : 0.12855294346809387\n",
      "\n",
      "\u001b[1;37mBatch loss 47 : 0.11525771766901016\n",
      "\n",
      "\u001b[1;37mBatch loss 48 : 0.14903715252876282\n",
      "\n",
      "\u001b[1;37mBatch loss 49 : 0.1206558421254158\n",
      "\n",
      "\u001b[1;37mBatch loss 50 : 0.11347785592079163\n",
      "\n",
      "\u001b[1;37mBatch loss 51 : 0.15433132648468018\n",
      "\n",
      "\u001b[1;37mBatch loss 52 : 0.13410839438438416\n",
      "\n",
      "\u001b[1;37mBatch loss 53 : 0.07917508482933044\n",
      "\n",
      "\u001b[1;37mBatch loss 54 : 0.1345222294330597\n",
      "\n",
      "\u001b[1;37mBatch loss 55 : 0.11112750321626663\n",
      "\n",
      "\u001b[1;37mBatch loss 56 : 0.07192519307136536\n",
      "\n",
      "\u001b[1;37mBatch loss 57 : 0.11004223674535751\n",
      "\n",
      "\u001b[1;37mBatch loss 58 : 0.11185136437416077\n",
      "\n",
      "\u001b[1;37mBatch loss 59 : 0.12493903934955597\n",
      "\n",
      "\u001b[1;37mBatch loss 60 : 0.1187015250325203\n",
      "\n",
      "\u001b[1;37mBatch loss 61 : 0.1638266146183014\n",
      "\n",
      "\u001b[1;37mBatch loss 62 : 0.06644756346940994\n",
      "\n",
      "\u001b[1;37mBatch loss 63 : 0.07199069857597351\n",
      "\n",
      "\u001b[1;37mBatch loss 64 : 0.07456731796264648\n",
      "\n",
      "\u001b[1;37mBatch loss 65 : 0.14123284816741943\n",
      "\n",
      "\u001b[1;37mBatch loss 66 : 0.07219462096691132\n",
      "\n",
      "\u001b[1;37mBatch loss 67 : 0.09274909645318985\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    EPOCHS = 1\n",
    "    LR = 1e-4\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, eps=5e-8)\n",
    "    module = train_loop(module=model,\n",
    "                        EPOCHS=EPOCHS, \n",
    "                        train_dataset=train_dataloaded, \n",
    "                        val_dataset=val_dataloaded,\n",
    "                        optimizer=optimizer)\n",
    "    predictions, true_targets = evaluate(module, \n",
    "                                         test_dataloaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'active.all.allocated': 2189543, 'active.all.current': 759, 'active.all.freed': 2188784, 'active.all.peak': 1024, 'active.large_pool.allocated': 1020003, 'active.large_pool.current': 277, 'active.large_pool.freed': 1019726, 'active.large_pool.peak': 381, 'active.small_pool.allocated': 1169540, 'active.small_pool.current': 482, 'active.small_pool.freed': 1169058, 'active.small_pool.peak': 721, 'active_bytes.all.allocated': 8415742742016, 'active_bytes.all.current': 10911057408, 'active_bytes.all.freed': 8404831684608, 'active_bytes.all.peak': 10917021184, 'active_bytes.large_pool.allocated': 8239346608128, 'active_bytes.large_pool.current': 10908502528, 'active_bytes.large_pool.freed': 8228438105600, 'active_bytes.large_pool.peak': 10913417728, 'active_bytes.small_pool.allocated': 176396133888, 'active_bytes.small_pool.current': 2554880, 'active_bytes.small_pool.freed': 176393579008, 'active_bytes.small_pool.peak': 13091328, 'allocated_bytes.all.allocated': 8415742742016, 'allocated_bytes.all.current': 10911057408, 'allocated_bytes.all.freed': 8404831684608, 'allocated_bytes.all.peak': 10917021184, 'allocated_bytes.large_pool.allocated': 8239346608128, 'allocated_bytes.large_pool.current': 10908502528, 'allocated_bytes.large_pool.freed': 8228438105600, 'allocated_bytes.large_pool.peak': 10913417728, 'allocated_bytes.small_pool.allocated': 176396133888, 'allocated_bytes.small_pool.current': 2554880, 'allocated_bytes.small_pool.freed': 176393579008, 'allocated_bytes.small_pool.peak': 13091328, 'allocation.all.allocated': 2189543, 'allocation.all.current': 759, 'allocation.all.freed': 2188784, 'allocation.all.peak': 1024, 'allocation.large_pool.allocated': 1020003, 'allocation.large_pool.current': 277, 'allocation.large_pool.freed': 1019726, 'allocation.large_pool.peak': 381, 'allocation.small_pool.allocated': 1169540, 'allocation.small_pool.current': 482, 'allocation.small_pool.freed': 1169058, 'allocation.small_pool.peak': 721, 'inactive_split.all.allocated': 923814, 'inactive_split.all.current': 109, 'inactive_split.all.freed': 923705, 'inactive_split.all.peak': 112, 'inactive_split.large_pool.allocated': 596745, 'inactive_split.large_pool.current': 107, 'inactive_split.large_pool.freed': 596638, 'inactive_split.large_pool.peak': 107, 'inactive_split.small_pool.allocated': 327069, 'inactive_split.small_pool.current': 2, 'inactive_split.small_pool.freed': 327067, 'inactive_split.small_pool.peak': 35, 'inactive_split_bytes.all.allocated': 7659926622208, 'inactive_split_bytes.all.current': 352745984, 'inactive_split_bytes.all.freed': 7659573876224, 'inactive_split_bytes.all.peak': 461697024, 'inactive_split_bytes.large_pool.allocated': 7477676526592, 'inactive_split_bytes.large_pool.current': 351106560, 'inactive_split_bytes.large_pool.freed': 7477325420032, 'inactive_split_bytes.large_pool.peak': 455366144, 'inactive_split_bytes.small_pool.allocated': 182250095616, 'inactive_split_bytes.small_pool.current': 1639424, 'inactive_split_bytes.small_pool.freed': 182248456192, 'inactive_split_bytes.small_pool.peak': 6462464, 'max_split_size': -1, 'num_alloc_retries': 2, 'num_device_alloc': 141, 'num_device_free': 34, 'num_ooms': 1, 'num_sync_all_streams': 2, 'oversize_allocations.allocated': 0, 'oversize_allocations.current': 0, 'oversize_allocations.freed': 0, 'oversize_allocations.peak': 0, 'oversize_segments.allocated': 0, 'oversize_segments.current': 0, 'oversize_segments.freed': 0, 'oversize_segments.peak': 0, 'requested_bytes.all.allocated': 8378264917645, 'requested_bytes.all.current': 10908486520, 'requested_bytes.all.freed': 8367356431125, 'requested_bytes.all.peak': 10914450296, 'requested_bytes.large_pool.allocated': 8202114798080, 'requested_bytes.large_pool.current': 10905956864, 'requested_bytes.large_pool.freed': 8191208841216, 'requested_bytes.large_pool.peak': 10910872064, 'requested_bytes.small_pool.allocated': 176150119565, 'requested_bytes.small_pool.current': 2529656, 'requested_bytes.small_pool.freed': 176147589909, 'requested_bytes.small_pool.peak': 13077692, 'reserved_bytes.all.allocated': 11863588864, 'reserved_bytes.all.current': 11263803392, 'reserved_bytes.all.freed': 599785472, 'reserved_bytes.all.peak': 11270094848, 'reserved_bytes.large_pool.allocated': 11846811648, 'reserved_bytes.large_pool.current': 11259609088, 'reserved_bytes.large_pool.freed': 587202560, 'reserved_bytes.large_pool.peak': 11259609088, 'reserved_bytes.small_pool.allocated': 16777216, 'reserved_bytes.small_pool.current': 4194304, 'reserved_bytes.small_pool.freed': 12582912, 'reserved_bytes.small_pool.peak': 14680064, 'segment.all.allocated': 141, 'segment.all.current': 107, 'segment.all.freed': 34, 'segment.all.peak': 137, 'segment.large_pool.allocated': 133, 'segment.large_pool.current': 105, 'segment.large_pool.freed': 28, 'segment.large_pool.peak': 130, 'segment.small_pool.allocated': 8, 'segment.small_pool.current': 2, 'segment.small_pool.freed': 6, 'segment.small_pool.peak': 7})\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.input_ids.max())\n",
    "print(inputs.input_ids.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_IODAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
