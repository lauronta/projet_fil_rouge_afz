{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "DATASET_PATH = Path(\"./data/text\")\n",
    "\n",
    "VOC_SIZE = 1000\n",
    "\n",
    "def load_data(datapath, max_size=None):\n",
    "    texts_files = list(datapath.glob(\"*.txt\"))\n",
    "    texts = []  \n",
    "    for files in texts_files:\n",
    "        with open(files, \"r\", encoding='utf8') as files:\n",
    "            text = files.readlines()\n",
    "            texts += text\n",
    "    texts = list(set(texts))\n",
    "    \n",
    "    return texts\n",
    "\n",
    "texts = load_data(DATASET_PATH)\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "# from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model_checkpoint = \"distilgpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', max_length=100\n",
    "                   , truncation=True, padding='max_length')\n",
    "\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "rand = torch.rand(inputs.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(mask_arr.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "\n",
    "selection[:5]\n",
    "\n",
    "for i in range(mask_arr.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103 # application du token [MASK]\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, idx):\n",
    "        self.encodings = encodings\n",
    "        self.idx = idx\n",
    "        self.encodings = {key: [val[i] for i in self.idx] for key, val in self.encodings.items()}\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "sample_idx = [i for i in range(len(inputs.input_ids))]\n",
    "\n",
    "shuffled_sample_idx = rd.sample(sample_idx, len(sample_idx))\n",
    "\n",
    "train_idx = shuffled_sample_idx[:int(0.70*len(shuffled_sample_idx))]\n",
    "val_idx = shuffled_sample_idx[int(0.70*len(shuffled_sample_idx)):int(0.85*len(shuffled_sample_idx))]\n",
    "test_idx = shuffled_sample_idx[int(0.85*len(shuffled_sample_idx)):]\n",
    "                                \n",
    "dataset_train = CustomDataset(inputs, train_idx)\n",
    "dataset_val = CustomDataset(inputs, val_idx)\n",
    "dataset_test = CustomDataset(inputs, test_idx)\n",
    "\n",
    "train_dataloaded = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "val_dataloaded = torch.utils.data.DataLoader(dataset_val, batch_size=16, shuffle=True)\n",
    "test_dataloaded = torch.utils.data.DataLoader(dataset_test, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLM_model(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(MLM_model, self).__init__()\n",
    "        self.history = {\"epochs\":[], \"test\":[]}\n",
    "        self.model = model\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.model.parameters()\n",
    "\n",
    "    def forward(self, x, attention_mask, labels):\n",
    "        return self.model(x, attention_mask, labels)\n",
    "    \n",
    "    def train_log(self, train_batch_losses, val_batch_losses, train_loss, validation_loss):\n",
    "        self.history[\"epochs\"].append({\"train_batch_losses\":train_batch_losses, \n",
    "                                \"val_batch_losses\":val_batch_losses, \n",
    "                                \"train_loss\":train_loss, \n",
    "                                \"validation_loss\":validation_loss})\n",
    "    \n",
    "    def test_log(self, test_batch_losses, test_loss):\n",
    "        self.history[\"test\"].append({\"test_batch_losses\":test_batch_losses,\n",
    "                                \"test_loss\":test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = MLM_model(model)\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(module, batch, batch_idx, optimizer):\n",
    "    module.train(True)\n",
    "    \n",
    "    inputs_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    outputs = module(inputs_ids, attention_mask, labels=labels)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    print(f\"\\n\\033[1;37mBatch loss {batch_idx+1} : {loss.item()}\")\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(module.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return module, loss\n",
    "\n",
    "def eval_step(module, batch, batch_idx, optimizer=None, training=True):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inputs_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        outputs = module(inputs_ids, attention_mask, labels=labels)\n",
    "    \n",
    "        loss = outputs.loss\n",
    "         \n",
    "        if training:\n",
    "            print(f\"\\n\\033[1;32mValidation Batch loss {batch_idx+1} : {loss.item()}\")\n",
    "            return module, loss\n",
    "        else:\n",
    "            print(f\"\\n\\033[1;32mTest Batch loss {batch_idx+1} : {loss.item()}\")\n",
    "            return module, loss, outputs, labels\n",
    "\n",
    "def train_loop(module, EPOCHS, train_dataset, val_dataset, optimizer, lr_scheduler=None):\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        module.train(True)\n",
    "        \n",
    "        train_batch_losses = []\n",
    "        for batch_idx in range(len(train_dataset)):\n",
    "            batch = next(iter(train_dataset))\n",
    "            module, loss = train_step(module, batch, batch_idx, optimizer)\n",
    "            train_batch_losses.append(loss.item())\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "          lr_scheduler.step()\n",
    "        train_loss = np.mean(train_batch_losses)\n",
    "\n",
    "        module.train(False)\n",
    "        val_batch_losses = []\n",
    "        for batch_idx in range(len(val_dataset)):\n",
    "            batch = next(iter(val_dataset))\n",
    "            module, loss = eval_step(module, batch, batch_idx)\n",
    "            val_batch_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "\n",
    "        module.train_log(train_batch_losses, val_batch_losses, train_loss, val_loss)\n",
    "        print(f\"\\n\\033[1;33mEpoch {epoch+1} :\\n\\033[1;37mTraining Loss : {train_loss}\")\n",
    "        print(f\"\\033[1;32mValidation Loss : {val_loss}\")\n",
    "    return module\n",
    "\n",
    "def evaluate(module, test_dataset):\n",
    "    module.train(False)\n",
    "    test_batch_losses = []\n",
    "    predictions = []\n",
    "    true_targets = []\n",
    "    for batch_idx in range(len(test_dataset)):\n",
    "        batch = next(iter(test_dataset))\n",
    "        module, loss, outputs, labels = eval_step(module, batch, batch_idx, training=False)\n",
    "\n",
    "        test_batch_losses.append(loss.item())\n",
    "        predictions.append(outputs)\n",
    "        true_targets.append(labels)\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    module.test_log(test_batch_losses, test_loss)\n",
    "    print(f\"\\nTest Loss : {test_loss}\")\n",
    "    return predictions, true_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateo\\AppData\\Local\\Temp\\ipykernel_18388\\3715340537.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;37mBatch loss 1 : 9.862822532653809\n",
      "\n",
      "\u001b[1;37mBatch loss 2 : 7.023928165435791\n",
      "\n",
      "\u001b[1;37mBatch loss 3 : 5.946075439453125\n",
      "\n",
      "\u001b[1;37mBatch loss 4 : 5.034688472747803\n",
      "\n",
      "\u001b[1;37mBatch loss 5 : 4.308846473693848\n",
      "\n",
      "\u001b[1;37mBatch loss 6 : 3.8753387928009033\n",
      "\n",
      "\u001b[1;37mBatch loss 7 : 3.112791061401367\n",
      "\n",
      "\u001b[1;37mBatch loss 8 : 2.5399646759033203\n",
      "\n",
      "\u001b[1;37mBatch loss 9 : 2.0797786712646484\n",
      "\n",
      "\u001b[1;37mBatch loss 10 : 1.767166256904602\n",
      "\n",
      "\u001b[1;37mBatch loss 11 : 1.4509884119033813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-8\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m module \u001b[38;5;241m=\u001b[39m train_loop(module\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m      7\u001b[0m                     EPOCHS\u001b[38;5;241m=\u001b[39mEPOCHS, \n\u001b[0;32m      8\u001b[0m                     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataloaded, \n\u001b[0;32m      9\u001b[0m                     val_dataset\u001b[38;5;241m=\u001b[39mval_dataloaded,\n\u001b[0;32m     10\u001b[0m                     optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[0;32m     11\u001b[0m predictions, true_targets \u001b[38;5;241m=\u001b[39m evaluate(module, \n\u001b[0;32m     12\u001b[0m                                      test_dataloaded, \n\u001b[0;32m     13\u001b[0m                                      nn\u001b[38;5;241m.\u001b[39mSmoothL1Loss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(module, EPOCHS, train_dataset, val_dataset, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m     45\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataset))\n\u001b[0;32m     46\u001b[0m     module, loss \u001b[38;5;241m=\u001b[39m train_step(module, batch, batch_idx, optimizer)\n\u001b[1;32m---> 47\u001b[0m     train_batch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m   lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    EPOCHS = 100\n",
    "    LR = 1e-4\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, eps=5e-8)\n",
    "    module = train_loop(module=model, \n",
    "                        EPOCHS=EPOCHS, \n",
    "                        train_dataset=train_dataloaded, \n",
    "                        val_dataset=val_dataloaded,\n",
    "                        optimizer=optimizer)\n",
    "    predictions, true_targets = evaluate(module, \n",
    "                                         test_dataloaded, \n",
    "                                         nn.SmoothL1Loss(reduction='mean'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29674)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids.max())\n",
    "print(inputs.input_ids.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pfr)",
   "language": "python",
   "name": "pfr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
