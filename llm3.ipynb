{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "DATASET_PATH = Path(\"./data/text\")\n",
    "\n",
    "VOC_SIZE = 1000\n",
    "\n",
    "def load_data(datapath, max_size=None):\n",
    "    texts_files = list(datapath.glob(\"*.txt\"))\n",
    "    texts = []  \n",
    "    for files in texts_files:\n",
    "        with open(files, \"r\") as files:\n",
    "            text = files.readlines()\n",
    "            texts += text\n",
    "    texts = list(set(texts))\n",
    "    return texts\n",
    "\n",
    "texts = load_data(DATASET_PATH)\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "# from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model_checkpoint = \"distilgpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', max_length=100\n",
    "                   , truncation=True, padding='max_length')\n",
    "\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "rand = torch.rand(inputs.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(mask_arr.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "\n",
    "selection[:5]\n",
    "\n",
    "for i in range(mask_arr.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103 # application du token [MASK]\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, idx):\n",
    "        self.encodings = encodings\n",
    "        self.idx = idx\n",
    "        self.encodings = {key: [val[i] for i in self.idx] for key, val in self.encodings.items()}\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "sample_idx = [i for i in range(len(inputs.input_ids))]\n",
    "\n",
    "shuffled_sample_idx = rd.sample(sample_idx, len(sample_idx))\n",
    "\n",
    "train_idx = shuffled_sample_idx[:int(0.70*len(shuffled_sample_idx))]\n",
    "val_idx = shuffled_sample_idx[int(0.70*len(shuffled_sample_idx)):int(0.85*len(shuffled_sample_idx))]\n",
    "test_idx = shuffled_sample_idx[int(0.85*len(shuffled_sample_idx)):]\n",
    "                                \n",
    "dataset_train = CustomDataset(inputs, train_idx)\n",
    "dataset_val = CustomDataset(inputs, val_idx)\n",
    "dataset_test = CustomDataset(inputs, test_idx)\n",
    "\n",
    "train_dataloaded = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "val_dataloaded = torch.utils.data.DataLoader(dataset_val, batch_size=16, shuffle=True)\n",
    "test_dataloaded = torch.utils.data.DataLoader(dataset_test, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class MLM_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLM_model, self).__init__()\n",
    "        self.history = {\"epochs\":[], \"test\":[]}\n",
    "\n",
    "    def forward(self, x):\n",
    "        return model(x)\n",
    "    \n",
    "    def train_log(self, train_batch_losses, val_batch_losses, train_loss, validation_loss):\n",
    "        self.history[\"epochs\"].append({\"train_batch_losses\":train_batch_losses, \n",
    "                                \"val_batch_losses\":val_batch_losses, \n",
    "                                \"train_loss\":train_loss, \n",
    "                                \"validation_loss\":validation_loss})\n",
    "    \n",
    "    def test_log(self, test_batch_losses, test_loss):\n",
    "        self.history[\"test\"].append({\"test_batch_losses\":test_batch_losses,\n",
    "                                \"test_loss\":test_loss})\n",
    "\n",
    "model = MLM_model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(module, batch, batch_idx, optimizer):\n",
    "    module.train(True)\n",
    "    \n",
    "    inputs_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    outputs = module(inputs_ids, attention_mask, labels=labels)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    print(f\"\\n\\033[1;37mBatch loss {batch_idx+1} : {loss.item()}\")\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(module.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return module, loss\n",
    "\n",
    "def eval_step(module, batch, batch_idx, optimizer=None, training=True):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inputs_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        outputs = module(inputs_ids, attention_mask, labels=labels)\n",
    "    \n",
    "        loss = outputs.loss\n",
    "         \n",
    "        if training:\n",
    "            print(f\"\\n\\033[1;32mValidation Batch loss {batch_idx+1} : {loss.item()}\")\n",
    "            return module, loss\n",
    "        else:\n",
    "            print(f\"\\n\\033[1;32mTest Batch loss {batch_idx+1} : {loss.item()}\")\n",
    "            return module, loss, outputs, labels\n",
    "\n",
    "def train_loop(module, EPOCHS, train_dataset, val_dataset, optimizer, lr_scheduler=None):\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        module.train(True)\n",
    "        \n",
    "        train_batch_losses = []\n",
    "        for batch_idx in range(len(train_dataset)):\n",
    "            batch = next(iter(train_dataset))\n",
    "            module, loss = train_step(module, batch, batch_idx, optimizer)\n",
    "            train_batch_losses.append(loss.item())\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "          lr_scheduler.step()\n",
    "        train_loss = np.mean(train_batch_losses)\n",
    "\n",
    "        module.train(False)\n",
    "        val_batch_losses = []\n",
    "        for batch_idx in range(len(val_dataset)):\n",
    "            batch = next(iter(val_dataset))\n",
    "            module, loss = eval_step(module, batch, batch_idx)\n",
    "            val_batch_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "\n",
    "        module.train_log(train_batch_losses, val_batch_losses, train_loss, val_loss)\n",
    "        print(f\"\\n\\033[1;33mEpoch {epoch+1} :\\n\\033[1;37mTraining Loss : {train_loss}\")\n",
    "        print(f\"\\033[1;32mValidation Loss : {val_loss}\")\n",
    "    return module\n",
    "\n",
    "def evaluate(module, test_dataset):\n",
    "    module.train(False)\n",
    "    test_batch_losses = []\n",
    "    predictions = []\n",
    "    true_targets = []\n",
    "    for batch_idx in range(len(test_dataset)):\n",
    "        batch = next(iter(test_dataset))\n",
    "        module, loss, outputs, labels = eval_step(module, batch, batch_idx, training=False)\n",
    "\n",
    "        test_batch_losses.append(loss.item())\n",
    "        predictions.append(outputs)\n",
    "        true_targets.append(labels)\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    module.test_log(test_batch_losses, test_loss)\n",
    "    print(f\"\\nTest Loss : {test_loss}\")\n",
    "    return predictions, true_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rapha\\AppData\\Local\\Temp\\ipykernel_38172\\3715340537.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;37mBatch loss 1 : 10.61193561553955\n",
      "\n",
      "\u001b[1;37mBatch loss 2 : 7.5935139656066895\n",
      "\n",
      "\u001b[1;37mBatch loss 3 : 6.683854579925537\n",
      "\n",
      "\u001b[1;37mBatch loss 4 : 4.44725227355957\n",
      "\n",
      "\u001b[1;37mBatch loss 5 : 3.9821932315826416\n",
      "\n",
      "\u001b[1;37mBatch loss 6 : 3.674833297729492\n",
      "\n",
      "\u001b[1;37mBatch loss 7 : 3.031693696975708\n",
      "\n",
      "\u001b[1;37mBatch loss 8 : 2.7303740978240967\n",
      "\n",
      "\u001b[1;37mBatch loss 9 : 2.252066135406494\n",
      "\n",
      "\u001b[1;37mBatch loss 10 : 1.5920186042785645\n",
      "\n",
      "\u001b[1;37mBatch loss 11 : 1.526757836341858\n",
      "\n",
      "\u001b[1;37mBatch loss 12 : 1.1759874820709229\n",
      "\n",
      "\u001b[1;37mBatch loss 13 : 0.8581132292747498\n",
      "\n",
      "\u001b[1;37mBatch loss 14 : 0.7906060218811035\n",
      "\n",
      "\u001b[1;37mBatch loss 15 : 0.778357982635498\n",
      "\n",
      "\u001b[1;37mBatch loss 16 : 0.6566455960273743\n",
      "\n",
      "\u001b[1;37mBatch loss 17 : 0.4071978032588959\n",
      "\n",
      "\u001b[1;37mBatch loss 18 : 0.3631610870361328\n",
      "\n",
      "\u001b[1;37mBatch loss 19 : 0.4673421084880829\n",
      "\n",
      "\u001b[1;37mBatch loss 20 : 0.3310314118862152\n",
      "\n",
      "\u001b[1;37mBatch loss 21 : 0.29903626441955566\n",
      "\n",
      "\u001b[1;37mBatch loss 22 : 0.20867100358009338\n",
      "\n",
      "\u001b[1;37mBatch loss 23 : 0.15724527835845947\n",
      "\n",
      "\u001b[1;37mBatch loss 24 : 0.14809587597846985\n",
      "\n",
      "\u001b[1;37mBatch loss 25 : 0.18980437517166138\n",
      "\n",
      "\u001b[1;37mBatch loss 26 : 0.19590166211128235\n",
      "\n",
      "\u001b[1;37mBatch loss 27 : 0.14288626611232758\n",
      "\n",
      "\u001b[1;37mBatch loss 28 : 0.13329541683197021\n",
      "\n",
      "\u001b[1;37mBatch loss 29 : 0.31305718421936035\n",
      "\n",
      "\u001b[1;37mBatch loss 30 : 0.15291593968868256\n",
      "\n",
      "\u001b[1;37mBatch loss 31 : 0.16753505170345306\n",
      "\n",
      "\u001b[1;37mBatch loss 32 : 0.19948361814022064\n",
      "\n",
      "\u001b[1;37mBatch loss 33 : 0.16031518578529358\n",
      "\n",
      "\u001b[1;37mBatch loss 34 : 0.1641112118959427\n",
      "\n",
      "\u001b[1;37mBatch loss 35 : 0.1806705892086029\n",
      "\n",
      "\u001b[1;37mBatch loss 36 : 0.14057236909866333\n",
      "\n",
      "\u001b[1;37mBatch loss 37 : 0.09679191559553146\n",
      "\n",
      "\u001b[1;37mBatch loss 38 : 0.11841931939125061\n",
      "\n",
      "\u001b[1;37mBatch loss 39 : 0.09046557545661926\n",
      "\n",
      "\u001b[1;37mBatch loss 40 : 0.09395089745521545\n",
      "\n",
      "\u001b[1;37mBatch loss 41 : 0.12707093358039856\n",
      "\n",
      "\u001b[1;37mBatch loss 42 : 0.12539036571979523\n",
      "\n",
      "\u001b[1;37mBatch loss 43 : 0.17116688191890717\n",
      "\n",
      "\u001b[1;37mBatch loss 44 : 0.1123923659324646\n",
      "\n",
      "\u001b[1;37mBatch loss 45 : 0.12845900654792786\n",
      "\n",
      "\u001b[1;37mBatch loss 46 : 0.13820448517799377\n",
      "\n",
      "\u001b[1;37mBatch loss 47 : 0.07892129570245743\n",
      "\n",
      "\u001b[1;37mBatch loss 48 : 0.09452521055936813\n",
      "\n",
      "\u001b[1;37mBatch loss 49 : 0.12742450833320618\n",
      "\n",
      "\u001b[1;37mBatch loss 50 : 0.09252221882343292\n",
      "\n",
      "\u001b[1;37mBatch loss 51 : 0.0798504427075386\n",
      "\n",
      "\u001b[1;37mBatch loss 52 : 0.13291068375110626\n",
      "\n",
      "\u001b[1;37mBatch loss 53 : 0.05573910102248192\n",
      "\n",
      "\u001b[1;37mBatch loss 54 : 0.1354227513074875\n",
      "\n",
      "\u001b[1;37mBatch loss 55 : 0.14398665726184845\n",
      "\n",
      "\u001b[1;37mBatch loss 56 : 0.10000033676624298\n",
      "\n",
      "\u001b[1;37mBatch loss 57 : 0.06995514035224915\n",
      "\n",
      "\u001b[1;37mBatch loss 58 : 0.10375598818063736\n",
      "\n",
      "\u001b[1;37mBatch loss 59 : 0.13779927790164948\n",
      "\n",
      "\u001b[1;37mBatch loss 60 : 0.13313959538936615\n",
      "\n",
      "\u001b[1;37mBatch loss 61 : 0.0590335875749588\n",
      "\n",
      "\u001b[1;37mBatch loss 62 : 0.11451953649520874\n",
      "\n",
      "\u001b[1;37mBatch loss 63 : 0.09368739277124405\n",
      "\n",
      "\u001b[1;37mBatch loss 64 : 0.14004795253276825\n",
      "\n",
      "\u001b[1;37mBatch loss 65 : 0.07285675406455994\n",
      "\n",
      "\u001b[1;37mBatch loss 66 : 0.06445736438035965\n",
      "\n",
      "\u001b[1;37mBatch loss 67 : 0.08856575936079025\n",
      "\n",
      "\u001b[1;37mBatch loss 68 : 0.12906266748905182\n",
      "\n",
      "\u001b[1;37mBatch loss 69 : 0.09165449440479279\n",
      "\n",
      "\u001b[1;37mBatch loss 70 : 0.11195605993270874\n",
      "\n",
      "\u001b[1;37mBatch loss 71 : 0.06826867908239365\n",
      "\n",
      "\u001b[1;37mBatch loss 72 : 0.10181321948766708\n",
      "\n",
      "\u001b[1;37mBatch loss 73 : 0.10572342574596405\n",
      "\n",
      "\u001b[1;37mBatch loss 74 : 0.04922686517238617\n",
      "\n",
      "\u001b[1;37mBatch loss 75 : 0.042848676443099976\n",
      "\n",
      "\u001b[1;37mBatch loss 76 : 0.1245771124958992\n",
      "\n",
      "\u001b[1;37mBatch loss 77 : 0.13425035774707794\n",
      "\n",
      "\u001b[1;37mBatch loss 78 : 0.1038844883441925\n",
      "\n",
      "\u001b[1;37mBatch loss 79 : 0.10567257553339005\n",
      "\n",
      "\u001b[1;37mBatch loss 80 : 0.056259285658597946\n",
      "\n",
      "\u001b[1;37mBatch loss 81 : 0.09655139595270157\n",
      "\n",
      "\u001b[1;37mBatch loss 82 : 0.12493382394313812\n",
      "\n",
      "\u001b[1;37mBatch loss 83 : 0.04329089820384979\n",
      "\n",
      "\u001b[1;37mBatch loss 84 : 0.11132915318012238\n",
      "\n",
      "\u001b[1;37mBatch loss 85 : 0.07785353809595108\n",
      "\n",
      "\u001b[1;37mBatch loss 86 : 0.05843449756503105\n",
      "\n",
      "\u001b[1;37mBatch loss 87 : 0.04811513051390648\n",
      "\n",
      "\u001b[1;37mBatch loss 88 : 0.057659443467855453\n",
      "\n",
      "\u001b[1;37mBatch loss 89 : 0.0547814816236496\n",
      "\n",
      "\u001b[1;37mBatch loss 90 : 0.0695343166589737\n",
      "\n",
      "\u001b[1;37mBatch loss 91 : 0.06863955408334732\n",
      "\n",
      "\u001b[1;37mBatch loss 92 : 0.10965695232152939\n",
      "\n",
      "\u001b[1;37mBatch loss 93 : 0.0737048089504242\n",
      "\n",
      "\u001b[1;37mBatch loss 94 : 0.022449878975749016\n",
      "\n",
      "\u001b[1;37mBatch loss 95 : 0.0890030562877655\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    EPOCHS = 100\n",
    "    LR = 1e-4\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, eps=5e-8)\n",
    "    module = train_loop(module=model, \n",
    "                        EPOCHS=EPOCHS, \n",
    "                        train_dataset=train_dataloaded, \n",
    "                        val_dataset=val_dataloaded,\n",
    "                        optimizer=optimizer)\n",
    "    predictions, true_targets = evaluate(module, \n",
    "                                         test_dataloaded, \n",
    "                                         nn.SmoothL1Loss(reduction='mean'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29674)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids.max())\n",
    "print(inputs.input_ids.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
