{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'cr', '##uc', '##ifer', '##es', ',', 'col', '##za', 'flora', '##ison', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import time\n",
    "import gc\n",
    "\n",
    "DATASET_PATH = Path(\"./data/text\")\n",
    "\n",
    "VOC_SIZE = 1000\n",
    "\n",
    "def load_data(datapath, max_size=None):\n",
    "    texts_files = list(datapath.glob(\"*.txt\"))\n",
    "    texts = []  \n",
    "    for files in texts_files:\n",
    "        with open(files, \"r\", encoding='utf8') as files:\n",
    "            text = files.readlines()\n",
    "            texts += text\n",
    "    texts = list(set(texts))\n",
    "    \n",
    "    return texts\n",
    "\n",
    "texts = load_data(DATASET_PATH)\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "# from transformers import AutoTokenizer\n",
    "from transformers import BertForMaskedLM, AutoTokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "tokenizerBERT_FT = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "modelBERT_FT = BertForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizerBERT_FT(texts, return_tensors='pt', max_length=100\n",
    "                   , truncation=True, padding='max_length')\n",
    "\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "print(inputs.tokens(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the [MASK] token with the mask array \n",
    "inputs.input_ids[mask_arr] = 103\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, idx):\n",
    "        self.encodings = encodings\n",
    "        self.idx = idx\n",
    "        self.encodings = {key: [val[i] for i in self.idx] for key, val in self.encodings.items()}\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "sample_idx = [i for i in range(len(inputs.input_ids))]\n",
    "\n",
    "shuffled_sample_idx = rd.sample(sample_idx, len(sample_idx))\n",
    "\n",
    "train_idx = shuffled_sample_idx[:int(0.70*len(shuffled_sample_idx))]\n",
    "val_idx = shuffled_sample_idx[int(0.70*len(shuffled_sample_idx)):int(0.85*len(shuffled_sample_idx))]\n",
    "test_idx = shuffled_sample_idx[int(0.85*len(shuffled_sample_idx)):]\n",
    "                                \n",
    "dataset_train = CustomDataset(inputs, train_idx)\n",
    "dataset_val = CustomDataset(inputs, val_idx)\n",
    "dataset_test = CustomDataset(inputs, test_idx)\n",
    "\n",
    "train_dataloaded = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "val_dataloaded = torch.utils.data.DataLoader(dataset_val, batch_size=16, shuffle=True)\n",
    "test_dataloaded = torch.utils.data.DataLoader(dataset_test, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MLM_model(nn.Module):\n",
    "#    def __init__(self, model):\n",
    "#        super(MLM_model, self).__init__()\n",
    "#        self.history = {\"epochs\":[], \"test\":[]}\n",
    "#        self.model = model\n",
    "    \n",
    "#    def parameters(self):\n",
    "#        return self.model.parameters()\n",
    "\n",
    "#    def forward(self, x, attention_mask, labels):\n",
    "#        return self.model(x, attention_mask, labels)\n",
    "    \n",
    "#    def train_log(self, train_batch_losses, val_batch_losses, train_loss, validation_loss):\n",
    "#        self.history[\"epochs\"].append({\"train_batch_losses\":train_batch_losses, \n",
    "#                                \"val_batch_losses\":val_batch_losses, \n",
    "#                                \"train_loss\":train_loss, \n",
    "#                                \"validation_loss\":validation_loss})\n",
    "    \n",
    "#    def test_log(self, test_batch_losses, test_loss):\n",
    "#        self.history[\"test\"].append({\"test_batch_losses\":test_batch_losses,\n",
    "#                                \"test_loss\":test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#model = MLM_model(model)\n",
    "modelBERT_FT.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(module, batch, batch_idx, optimizer):\n",
    "    module.train(True)\n",
    "    \n",
    "    inputs_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    outputs = module(inputs_ids, attention_mask, labels=labels)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    print(f\"\\n\\033[1;37mBatch loss {batch_idx+1} : {loss.item()}\")\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(module.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return module, loss\n",
    "\n",
    "def eval_step(module, batch, batch_idx, optimizer=None, training=True):\n",
    "    if training == False :\n",
    "        module.to('cpu')\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inputs_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        outputs = module(inputs_ids, attention_mask, labels=labels)\n",
    "    \n",
    "        loss = outputs.loss\n",
    "         \n",
    "        if training:\n",
    "            print(f\"\\n\\033[1;32mValidation Batch loss {batch_idx+1} : {loss.item()}\")\n",
    "            return module, loss\n",
    "        else:\n",
    "            print(f\"\\n\\033[1;32mTest Batch loss {batch_idx+1} : {loss.item()}\")\n",
    "            return module, loss, outputs, labels\n",
    "\n",
    "def train_loop(module, EPOCHS, train_dataset, val_dataset, optimizer, lr_scheduler=None):\n",
    "    for epoch in range(EPOCHS):\n",
    "        deb=time.time()\n",
    "        \n",
    "        module.train(True)\n",
    "        \n",
    "        train_batch_losses = []\n",
    "        for batch_idx in range(len(train_dataset)):\n",
    "            batch = next(iter(train_dataset))\n",
    "            module, loss = train_step(module, batch, batch_idx, optimizer)\n",
    "            train_batch_losses.append(loss.item())\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "          lr_scheduler.step()\n",
    "        train_loss = np.mean(train_batch_losses)\n",
    "\n",
    "        module.train(False)\n",
    "        val_batch_losses = []\n",
    "        for batch_idx in range(len(val_dataset)):\n",
    "            batch = next(iter(val_dataset))\n",
    "            module, loss = eval_step(module, batch, batch_idx)\n",
    "            val_batch_losses.append(loss.item())\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "\n",
    "#        module.train_log(train_batch_losses, val_batch_losses, train_loss, val_loss)\n",
    "        print(f\"\\n\\033[1;33mEpoch {epoch+1} :\\n\\033[1;37mTraining Loss : {train_loss}\")\n",
    "        print(f\"\\033[1;32mValidation Loss : {val_loss}\")\n",
    "        print(f\"\\033[1;31mDurÃ©e epoch : {time.time()-deb} secondes\")\n",
    "    return module\n",
    "\n",
    "def evaluate(module, test_dataset):\n",
    "    module.train(False)\n",
    "    test_batch_losses = []\n",
    "    predictions = []\n",
    "    true_targets = []\n",
    "    for batch_idx in range(len(test_dataset)):\n",
    "        batch = next(iter(test_dataset))\n",
    "        module, loss, outputs, labels = eval_step(module, batch, batch_idx, training=False)\n",
    "\n",
    "        test_batch_losses.append(loss.item())\n",
    "        predictions.append(outputs)\n",
    "        true_targets.append(labels)\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "#    module.test_log(test_batch_losses, test_loss)\n",
    "    print(f\"\\nTest Loss : {test_loss}\")\n",
    "    return predictions, true_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateo\\AppData\\Local\\Temp\\ipykernel_15588\\2355751477.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;37mBatch loss 1 : 10.015976905822754\n",
      "\n",
      "\u001b[1;37mBatch loss 2 : 7.16989803314209\n",
      "\n",
      "\u001b[1;37mBatch loss 3 : 5.97263240814209\n",
      "\n",
      "\u001b[1;37mBatch loss 4 : 5.151910781860352\n",
      "\n",
      "\u001b[1;37mBatch loss 5 : 3.9510135650634766\n",
      "\n",
      "\u001b[1;37mBatch loss 6 : 3.5560452938079834\n",
      "\n",
      "\u001b[1;37mBatch loss 7 : 3.0861096382141113\n",
      "\n",
      "\u001b[1;37mBatch loss 8 : 2.552812099456787\n",
      "\n",
      "\u001b[1;37mBatch loss 9 : 2.1852333545684814\n",
      "\n",
      "\u001b[1;37mBatch loss 10 : 1.7060927152633667\n",
      "\n",
      "\u001b[1;37mBatch loss 11 : 1.5518842935562134\n",
      "\n",
      "\u001b[1;37mBatch loss 12 : 1.2483863830566406\n",
      "\n",
      "\u001b[1;37mBatch loss 13 : 0.9671221375465393\n",
      "\n",
      "\u001b[1;37mBatch loss 14 : 0.7905038595199585\n",
      "\n",
      "\u001b[1;37mBatch loss 15 : 0.658480167388916\n",
      "\n",
      "\u001b[1;37mBatch loss 16 : 0.4967593252658844\n",
      "\n",
      "\u001b[1;37mBatch loss 17 : 0.40568307042121887\n",
      "\n",
      "\u001b[1;37mBatch loss 18 : 0.41909778118133545\n",
      "\n",
      "\u001b[1;37mBatch loss 19 : 0.38154059648513794\n",
      "\n",
      "\u001b[1;37mBatch loss 20 : 0.32720473408699036\n",
      "\n",
      "\u001b[1;37mBatch loss 21 : 0.2920907437801361\n",
      "\n",
      "\u001b[1;37mBatch loss 22 : 0.3172966241836548\n",
      "\n",
      "\u001b[1;37mBatch loss 23 : 0.2509572207927704\n",
      "\n",
      "\u001b[1;37mBatch loss 24 : 0.22534820437431335\n",
      "\n",
      "\u001b[1;37mBatch loss 25 : 0.15022319555282593\n",
      "\n",
      "\u001b[1;37mBatch loss 26 : 0.17015211284160614\n",
      "\n",
      "\u001b[1;37mBatch loss 27 : 0.17785394191741943\n",
      "\n",
      "\u001b[1;37mBatch loss 28 : 0.23631872236728668\n",
      "\n",
      "\u001b[1;37mBatch loss 29 : 0.1758802831172943\n",
      "\n",
      "\u001b[1;37mBatch loss 30 : 0.1972787231206894\n",
      "\n",
      "\u001b[1;37mBatch loss 31 : 0.13899344205856323\n",
      "\n",
      "\u001b[1;37mBatch loss 32 : 0.11762157082557678\n",
      "\n",
      "\u001b[1;37mBatch loss 33 : 0.17821355164051056\n",
      "\n",
      "\u001b[1;37mBatch loss 34 : 0.11821150779724121\n",
      "\n",
      "\u001b[1;37mBatch loss 35 : 0.15442650020122528\n",
      "\n",
      "\u001b[1;37mBatch loss 36 : 0.18240541219711304\n",
      "\n",
      "\u001b[1;37mBatch loss 37 : 0.10844232887029648\n",
      "\n",
      "\u001b[1;37mBatch loss 38 : 0.12831711769104004\n",
      "\n",
      "\u001b[1;37mBatch loss 39 : 0.10643517225980759\n",
      "\n",
      "\u001b[1;37mBatch loss 40 : 0.0770849660038948\n",
      "\n",
      "\u001b[1;37mBatch loss 41 : 0.114201620221138\n",
      "\n",
      "\u001b[1;37mBatch loss 42 : 0.12953564524650574\n",
      "\n",
      "\u001b[1;37mBatch loss 43 : 0.10253021121025085\n",
      "\n",
      "\u001b[1;37mBatch loss 44 : 0.20610041916370392\n",
      "\n",
      "\u001b[1;37mBatch loss 45 : 0.07973114401102066\n",
      "\n",
      "\u001b[1;37mBatch loss 46 : 0.13376584649085999\n",
      "\n",
      "\u001b[1;37mBatch loss 47 : 0.18307504057884216\n",
      "\n",
      "\u001b[1;37mBatch loss 48 : 0.0949198305606842\n",
      "\n",
      "\u001b[1;37mBatch loss 49 : 0.10488539934158325\n",
      "\n",
      "\u001b[1;37mBatch loss 50 : 0.08308795839548111\n",
      "\n",
      "\u001b[1;37mBatch loss 51 : 0.1294739842414856\n",
      "\n",
      "\u001b[1;37mBatch loss 52 : 0.1593739092350006\n",
      "\n",
      "\u001b[1;37mBatch loss 53 : 0.1519775241613388\n",
      "\n",
      "\u001b[1;37mBatch loss 54 : 0.15734358131885529\n",
      "\n",
      "\u001b[1;37mBatch loss 55 : 0.08860982954502106\n",
      "\n",
      "\u001b[1;37mBatch loss 56 : 0.11901859194040298\n",
      "\n",
      "\u001b[1;37mBatch loss 57 : 0.1014552041888237\n",
      "\n",
      "\u001b[1;37mBatch loss 58 : 0.15223875641822815\n",
      "\n",
      "\u001b[1;37mBatch loss 59 : 0.12375184893608093\n",
      "\n",
      "\u001b[1;37mBatch loss 60 : 0.11338716745376587\n",
      "\n",
      "\u001b[1;37mBatch loss 61 : 0.09365780651569366\n",
      "\n",
      "\u001b[1;37mBatch loss 62 : 0.08451101183891296\n",
      "\n",
      "\u001b[1;37mBatch loss 63 : 0.07848349958658218\n",
      "\n",
      "\u001b[1;37mBatch loss 64 : 0.17405939102172852\n",
      "\n",
      "\u001b[1;37mBatch loss 65 : 0.1341858059167862\n",
      "\n",
      "\u001b[1;37mBatch loss 66 : 0.07510451972484589\n",
      "\n",
      "\u001b[1;37mBatch loss 67 : 0.12220925092697144\n",
      "\n",
      "\u001b[1;37mBatch loss 68 : 0.06423614919185638\n",
      "\n",
      "\u001b[1;37mBatch loss 69 : 0.10638436675071716\n",
      "\n",
      "\u001b[1;37mBatch loss 70 : 0.14190584421157837\n",
      "\n",
      "\u001b[1;37mBatch loss 71 : 0.09512264281511307\n",
      "\n",
      "\u001b[1;37mBatch loss 72 : 0.08805696666240692\n",
      "\n",
      "\u001b[1;37mBatch loss 73 : 0.11400957405567169\n",
      "\n",
      "\u001b[1;37mBatch loss 74 : 0.12769779562950134\n",
      "\n",
      "\u001b[1;37mBatch loss 75 : 0.11937787383794785\n",
      "\n",
      "\u001b[1;37mBatch loss 76 : 0.09915560483932495\n",
      "\n",
      "\u001b[1;37mBatch loss 77 : 0.1066780760884285\n",
      "\n",
      "\u001b[1;37mBatch loss 78 : 0.05598241835832596\n",
      "\n",
      "\u001b[1;37mBatch loss 79 : 0.07983428984880447\n",
      "\n",
      "\u001b[1;37mBatch loss 80 : 0.09140779823064804\n",
      "\n",
      "\u001b[1;37mBatch loss 81 : 0.091720812022686\n",
      "\n",
      "\u001b[1;37mBatch loss 82 : 0.1540258526802063\n",
      "\n",
      "\u001b[1;37mBatch loss 83 : 0.0968555137515068\n",
      "\n",
      "\u001b[1;37mBatch loss 84 : 0.08073970675468445\n",
      "\n",
      "\u001b[1;37mBatch loss 85 : 0.06645533442497253\n",
      "\n",
      "\u001b[1;37mBatch loss 86 : 0.07934096455574036\n",
      "\n",
      "\u001b[1;37mBatch loss 87 : 0.08578141033649445\n",
      "\n",
      "\u001b[1;37mBatch loss 88 : 0.08759941160678864\n",
      "\n",
      "\u001b[1;37mBatch loss 89 : 0.07828278839588165\n",
      "\n",
      "\u001b[1;37mBatch loss 90 : 0.0732947438955307\n",
      "\n",
      "\u001b[1;37mBatch loss 91 : 0.05908208340406418\n",
      "\n",
      "\u001b[1;37mBatch loss 92 : 0.08659867197275162\n",
      "\n",
      "\u001b[1;37mBatch loss 93 : 0.057636089622974396\n",
      "\n",
      "\u001b[1;37mBatch loss 94 : 0.15621742606163025\n",
      "\n",
      "\u001b[1;37mBatch loss 95 : 0.09120997786521912\n",
      "\n",
      "\u001b[1;37mBatch loss 96 : 0.17131994664669037\n",
      "\n",
      "\u001b[1;37mBatch loss 97 : 0.0665372833609581\n",
      "\n",
      "\u001b[1;37mBatch loss 98 : 0.09665844589471817\n",
      "\n",
      "\u001b[1;37mBatch loss 99 : 0.14422562718391418\n",
      "\n",
      "\u001b[1;37mBatch loss 100 : 0.07989692687988281\n",
      "\n",
      "\u001b[1;37mBatch loss 101 : 0.09162409603595734\n",
      "\n",
      "\u001b[1;37mBatch loss 102 : 0.10918010026216507\n",
      "\n",
      "\u001b[1;37mBatch loss 103 : 0.10012200474739075\n",
      "\n",
      "\u001b[1;37mBatch loss 104 : 0.05918017774820328\n",
      "\n",
      "\u001b[1;37mBatch loss 105 : 0.08066905289888382\n",
      "\n",
      "\u001b[1;37mBatch loss 106 : 0.10029903054237366\n",
      "\n",
      "\u001b[1;37mBatch loss 107 : 0.12910136580467224\n",
      "\n",
      "\u001b[1;37mBatch loss 108 : 0.1078399196267128\n",
      "\n",
      "\u001b[1;37mBatch loss 109 : 0.08902178704738617\n",
      "\n",
      "\u001b[1;37mBatch loss 110 : 0.03572060912847519\n",
      "\n",
      "\u001b[1;37mBatch loss 111 : 0.075046606361866\n",
      "\n",
      "\u001b[1;37mBatch loss 112 : 0.07297458499670029\n",
      "\n",
      "\u001b[1;37mBatch loss 113 : 0.08762022852897644\n",
      "\n",
      "\u001b[1;37mBatch loss 114 : 0.11558163911104202\n",
      "\n",
      "\u001b[1;37mBatch loss 115 : 0.15401571989059448\n",
      "\n",
      "\u001b[1;37mBatch loss 116 : 0.07415745407342911\n",
      "\n",
      "\u001b[1;37mBatch loss 117 : 0.1621086448431015\n",
      "\n",
      "\u001b[1;37mBatch loss 118 : 0.061789706349372864\n",
      "\n",
      "\u001b[1;37mBatch loss 119 : 0.07171180099248886\n",
      "\n",
      "\u001b[1;37mBatch loss 120 : 0.06558994203805923\n",
      "\n",
      "\u001b[1;37mBatch loss 121 : 0.11414142698049545\n",
      "\n",
      "\u001b[1;37mBatch loss 122 : 0.16951023042201996\n",
      "\n",
      "\u001b[1;37mBatch loss 123 : 0.05261167511343956\n",
      "\n",
      "\u001b[1;37mBatch loss 124 : 0.11190846562385559\n",
      "\n",
      "\u001b[1;37mBatch loss 125 : 0.05294293537735939\n",
      "\n",
      "\u001b[1;37mBatch loss 126 : 0.04070867970585823\n",
      "\n",
      "\u001b[1;37mBatch loss 127 : 0.07241380959749222\n",
      "\n",
      "\u001b[1;37mBatch loss 128 : 0.039421308785676956\n",
      "\n",
      "\u001b[1;37mBatch loss 129 : 0.0630558654665947\n",
      "\n",
      "\u001b[1;37mBatch loss 130 : 0.04496899992227554\n",
      "\n",
      "\u001b[1;37mBatch loss 131 : 0.13313397765159607\n",
      "\n",
      "\u001b[1;37mBatch loss 132 : 0.05649619549512863\n",
      "\n",
      "\u001b[1;37mBatch loss 133 : 0.06603460013866425\n",
      "\n",
      "\u001b[1;37mBatch loss 134 : 0.09267167001962662\n",
      "\n",
      "\u001b[1;37mBatch loss 135 : 0.06406921148300171\n",
      "\n",
      "\u001b[1;37mBatch loss 136 : 0.08126435428857803\n",
      "\n",
      "\u001b[1;37mBatch loss 137 : 0.0811636745929718\n",
      "\n",
      "\u001b[1;37mBatch loss 138 : 0.12187953293323517\n",
      "\n",
      "\u001b[1;37mBatch loss 139 : 0.07063388079404831\n",
      "\n",
      "\u001b[1;37mBatch loss 140 : 0.1010146513581276\n",
      "\n",
      "\u001b[1;37mBatch loss 141 : 0.06789602339267731\n",
      "\n",
      "\u001b[1;37mBatch loss 142 : 0.04570193588733673\n",
      "\n",
      "\u001b[1;37mBatch loss 143 : 0.06343772262334824\n",
      "\n",
      "\u001b[1;37mBatch loss 144 : 0.061465974897146225\n",
      "\n",
      "\u001b[1;37mBatch loss 145 : 0.0593436136841774\n",
      "\n",
      "\u001b[1;37mBatch loss 146 : 0.11357998847961426\n",
      "\n",
      "\u001b[1;37mBatch loss 147 : 0.0817006304860115\n",
      "\n",
      "\u001b[1;37mBatch loss 148 : 0.10596384108066559\n",
      "\n",
      "\u001b[1;37mBatch loss 149 : 0.09828729927539825\n",
      "\n",
      "\u001b[1;37mBatch loss 150 : 0.06645791232585907\n",
      "\n",
      "\u001b[1;37mBatch loss 151 : 0.06923215836286545\n",
      "\n",
      "\u001b[1;37mBatch loss 152 : 0.11955138295888901\n",
      "\n",
      "\u001b[1;37mBatch loss 153 : 0.08911256492137909\n",
      "\n",
      "\u001b[1;37mBatch loss 154 : 0.09964151680469513\n",
      "\n",
      "\u001b[1;37mBatch loss 155 : 0.05349741876125336\n",
      "\n",
      "\u001b[1;37mBatch loss 156 : 0.11260022222995758\n",
      "\n",
      "\u001b[1;37mBatch loss 157 : 0.13303101062774658\n",
      "\n",
      "\u001b[1;37mBatch loss 158 : 0.09170524775981903\n",
      "\n",
      "\u001b[1;37mBatch loss 159 : 0.0710059180855751\n",
      "\n",
      "\u001b[1;37mBatch loss 160 : 0.045368488878011703\n",
      "\n",
      "\u001b[1;37mBatch loss 161 : 0.031050994992256165\n",
      "\n",
      "\u001b[1;37mBatch loss 162 : 0.03186710178852081\n",
      "\n",
      "\u001b[1;37mBatch loss 163 : 0.1083870381116867\n",
      "\n",
      "\u001b[1;37mBatch loss 164 : 0.08763983100652695\n",
      "\n",
      "\u001b[1;37mBatch loss 165 : 0.05345475301146507\n",
      "\n",
      "\u001b[1;37mBatch loss 166 : 0.05057249963283539\n",
      "\n",
      "\u001b[1;37mBatch loss 167 : 0.06371286511421204\n",
      "\n",
      "\u001b[1;37mBatch loss 168 : 0.06124060973525047\n",
      "\n",
      "\u001b[1;37mBatch loss 169 : 0.04961983114480972\n",
      "\n",
      "\u001b[1;37mBatch loss 170 : 0.15630513429641724\n",
      "\n",
      "\u001b[1;37mBatch loss 171 : 0.0959801897406578\n",
      "\n",
      "\u001b[1;37mBatch loss 172 : 0.08749532699584961\n",
      "\n",
      "\u001b[1;37mBatch loss 173 : 0.09418535977602005\n",
      "\n",
      "\u001b[1;37mBatch loss 174 : 0.0896899402141571\n",
      "\n",
      "\u001b[1;37mBatch loss 175 : 0.035347018390893936\n",
      "\n",
      "\u001b[1;37mBatch loss 176 : 0.04407458007335663\n",
      "\n",
      "\u001b[1;37mBatch loss 177 : 0.03921303525567055\n",
      "\n",
      "\u001b[1;37mBatch loss 178 : 0.07051868736743927\n",
      "\n",
      "\u001b[1;37mBatch loss 179 : 0.05475877597928047\n",
      "\n",
      "\u001b[1;37mBatch loss 180 : 0.024548042565584183\n",
      "\n",
      "\u001b[1;37mBatch loss 181 : 0.03493852540850639\n",
      "\n",
      "\u001b[1;37mBatch loss 182 : 0.07235109061002731\n",
      "\n",
      "\u001b[1;37mBatch loss 183 : 0.08622732758522034\n",
      "\n",
      "\u001b[1;37mBatch loss 184 : 0.060202375054359436\n",
      "\n",
      "\u001b[1;37mBatch loss 185 : 0.02141530066728592\n",
      "\n",
      "\u001b[1;37mBatch loss 186 : 0.05466170236468315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;37mBatch loss 187 : 0.04398718848824501\n",
      "\n",
      "\u001b[1;37mBatch loss 188 : 0.04692811146378517\n",
      "\n",
      "\u001b[1;37mBatch loss 189 : 0.09959032386541367\n",
      "\n",
      "\u001b[1;37mBatch loss 190 : 0.06682781875133514\n",
      "\n",
      "\u001b[1;37mBatch loss 191 : 0.0643148198723793\n",
      "\n",
      "\u001b[1;37mBatch loss 192 : 0.05941101163625717\n",
      "\n",
      "\u001b[1;37mBatch loss 193 : 0.03663650527596474\n",
      "\n",
      "\u001b[1;37mBatch loss 194 : 0.05901409685611725\n",
      "\n",
      "\u001b[1;37mBatch loss 195 : 0.06762278079986572\n",
      "\n",
      "\u001b[1;37mBatch loss 196 : 0.07380421459674835\n",
      "\n",
      "\u001b[1;37mBatch loss 197 : 0.06400299817323685\n",
      "\n",
      "\u001b[1;37mBatch loss 198 : 0.09313797205686569\n",
      "\n",
      "\u001b[1;37mBatch loss 199 : 0.04881320893764496\n",
      "\n",
      "\u001b[1;37mBatch loss 200 : 0.0973479300737381\n",
      "\n",
      "\u001b[1;37mBatch loss 201 : 0.05228410288691521\n",
      "\n",
      "\u001b[1;37mBatch loss 202 : 0.09519591927528381\n",
      "\n",
      "\u001b[1;37mBatch loss 203 : 0.07802216708660126\n",
      "\n",
      "\u001b[1;37mBatch loss 204 : 0.06779863685369492\n",
      "\n",
      "\u001b[1;37mBatch loss 205 : 0.04498574882745743\n",
      "\n",
      "\u001b[1;37mBatch loss 206 : 0.05348963662981987\n",
      "\n",
      "\u001b[1;37mBatch loss 207 : 0.0867738127708435\n",
      "\n",
      "\u001b[1;37mBatch loss 208 : 0.08564994484186172\n",
      "\n",
      "\u001b[1;37mBatch loss 209 : 0.09044007956981659\n",
      "\n",
      "\u001b[1;37mBatch loss 210 : 0.08469443023204803\n",
      "\n",
      "\u001b[1;37mBatch loss 211 : 0.06738990545272827\n",
      "\n",
      "\u001b[1;37mBatch loss 212 : 0.06215418502688408\n",
      "\n",
      "\u001b[1;37mBatch loss 213 : 0.06510617583990097\n",
      "\n",
      "\u001b[1;37mBatch loss 214 : 0.07139807939529419\n",
      "\n",
      "\u001b[1;37mBatch loss 215 : 0.03052685409784317\n",
      "\n",
      "\u001b[1;37mBatch loss 216 : 0.07280860841274261\n",
      "\n",
      "\u001b[1;37mBatch loss 217 : 0.033554088324308395\n",
      "\n",
      "\u001b[1;37mBatch loss 218 : 0.05574294924736023\n",
      "\n",
      "\u001b[1;37mBatch loss 219 : 0.07814913988113403\n",
      "\n",
      "\u001b[1;37mBatch loss 220 : 0.026294713839888573\n",
      "\n",
      "\u001b[1;37mBatch loss 221 : 0.04639671370387077\n",
      "\n",
      "\u001b[1;37mBatch loss 222 : 0.06903577595949173\n",
      "\n",
      "\u001b[1;37mBatch loss 223 : 0.061028119176626205\n",
      "\n",
      "\u001b[1;37mBatch loss 224 : 0.07261169701814651\n",
      "\n",
      "\u001b[1;37mBatch loss 225 : 0.06329574435949326\n",
      "\n",
      "\u001b[1;37mBatch loss 226 : 0.10994162410497665\n",
      "\n",
      "\u001b[1;37mBatch loss 227 : 0.1027817502617836\n",
      "\n",
      "\u001b[1;37mBatch loss 228 : 0.04751851409673691\n",
      "\n",
      "\u001b[1;37mBatch loss 229 : 0.05337299779057503\n",
      "\n",
      "\u001b[1;37mBatch loss 230 : 0.034956127405166626\n",
      "\n",
      "\u001b[1;37mBatch loss 231 : 0.09407296776771545\n",
      "\n",
      "\u001b[1;37mBatch loss 232 : 0.04465356841683388\n",
      "\n",
      "\u001b[1;37mBatch loss 233 : 0.06271092593669891\n",
      "\n",
      "\u001b[1;37mBatch loss 234 : 0.06341692805290222\n",
      "\n",
      "\u001b[1;37mBatch loss 235 : 0.06935486197471619\n",
      "\n",
      "\u001b[1;37mBatch loss 236 : 0.04528043419122696\n",
      "\n",
      "\u001b[1;37mBatch loss 237 : 0.08055323362350464\n",
      "\n",
      "\u001b[1;37mBatch loss 238 : 0.06083931773900986\n",
      "\n",
      "\u001b[1;37mBatch loss 239 : 0.04526295140385628\n",
      "\n",
      "\u001b[1;37mBatch loss 240 : 0.028959862887859344\n",
      "\n",
      "\u001b[1;37mBatch loss 241 : 0.1041070967912674\n",
      "\n",
      "\u001b[1;37mBatch loss 242 : 0.03315066173672676\n",
      "\n",
      "\u001b[1;37mBatch loss 243 : 0.050445254892110825\n",
      "\n",
      "\u001b[1;37mBatch loss 244 : 0.06226399913430214\n",
      "\n",
      "\u001b[1;37mBatch loss 245 : 0.1109263226389885\n",
      "\n",
      "\u001b[1;37mBatch loss 246 : 0.08294489979743958\n",
      "\n",
      "\u001b[1;37mBatch loss 247 : 0.05778050422668457\n",
      "\n",
      "\u001b[1;37mBatch loss 248 : 0.10595444589853287\n",
      "\n",
      "\u001b[1;37mBatch loss 249 : 0.0872129425406456\n",
      "\n",
      "\u001b[1;37mBatch loss 250 : 0.08511710911989212\n",
      "\n",
      "\u001b[1;37mBatch loss 251 : 0.06727562099695206\n",
      "\n",
      "\u001b[1;37mBatch loss 252 : 0.03826835751533508\n",
      "\n",
      "\u001b[1;37mBatch loss 253 : 0.04224588721990585\n",
      "\n",
      "\u001b[1;37mBatch loss 254 : 0.07603505998849869\n",
      "\n",
      "\u001b[1;37mBatch loss 255 : 0.04867583140730858\n",
      "\n",
      "\u001b[1;37mBatch loss 256 : 0.07262936234474182\n",
      "\n",
      "\u001b[1;37mBatch loss 257 : 0.03471793606877327\n",
      "\n",
      "\u001b[1;37mBatch loss 258 : 0.10778816044330597\n",
      "\n",
      "\u001b[1;37mBatch loss 259 : 0.05938095226883888\n",
      "\n",
      "\u001b[1;37mBatch loss 260 : 0.040308866649866104\n",
      "\n",
      "\u001b[1;37mBatch loss 261 : 0.07834619283676147\n",
      "\n",
      "\u001b[1;37mBatch loss 262 : 0.059297699481248856\n",
      "\n",
      "\u001b[1;37mBatch loss 263 : 0.0733218714594841\n",
      "\n",
      "\u001b[1;37mBatch loss 264 : 0.06987947225570679\n",
      "\n",
      "\u001b[1;37mBatch loss 265 : 0.03127874433994293\n",
      "\n",
      "\u001b[1;37mBatch loss 266 : 0.0703716054558754\n",
      "\n",
      "\u001b[1;37mBatch loss 267 : 0.07658005505800247\n",
      "\n",
      "\u001b[1;37mBatch loss 268 : 0.13312719762325287\n",
      "\n",
      "\u001b[1;37mBatch loss 269 : 0.0889369398355484\n",
      "\n",
      "\u001b[1;37mBatch loss 270 : 0.03802015259861946\n",
      "\n",
      "\u001b[1;37mBatch loss 271 : 0.03947989642620087\n",
      "\n",
      "\u001b[1;37mBatch loss 272 : 0.06150711700320244\n",
      "\n",
      "\u001b[1;37mBatch loss 273 : 0.07746441662311554\n",
      "\n",
      "\u001b[1;37mBatch loss 274 : 0.06046338006854057\n",
      "\n",
      "\u001b[1;37mBatch loss 275 : 0.043418072164058685\n",
      "\n",
      "\u001b[1;37mBatch loss 276 : 0.07823171466588974\n",
      "\n",
      "\u001b[1;37mBatch loss 277 : 0.05760293826460838\n",
      "\n",
      "\u001b[1;37mBatch loss 278 : 0.03294333070516586\n",
      "\n",
      "\u001b[1;37mBatch loss 279 : 0.04280146583914757\n",
      "\n",
      "\u001b[1;37mBatch loss 280 : 0.0647178590297699\n",
      "\n",
      "\u001b[1;37mBatch loss 281 : 0.07773954421281815\n",
      "\n",
      "\u001b[1;37mBatch loss 282 : 0.08106771111488342\n",
      "\n",
      "\u001b[1;37mBatch loss 283 : 0.034160301089286804\n",
      "\n",
      "\u001b[1;37mBatch loss 284 : 0.08227372169494629\n",
      "\n",
      "\u001b[1;37mBatch loss 285 : 0.07017126679420471\n",
      "\n",
      "\u001b[1;37mBatch loss 286 : 0.07470803707838058\n",
      "\n",
      "\u001b[1;37mBatch loss 287 : 0.07842759788036346\n",
      "\n",
      "\u001b[1;37mBatch loss 288 : 0.05372472107410431\n",
      "\n",
      "\u001b[1;37mBatch loss 289 : 0.04263058677315712\n",
      "\n",
      "\u001b[1;37mBatch loss 290 : 0.04540179297327995\n",
      "\n",
      "\u001b[1;37mBatch loss 291 : 0.04210621863603592\n",
      "\n",
      "\u001b[1;37mBatch loss 292 : 0.07460808753967285\n",
      "\n",
      "\u001b[1;37mBatch loss 293 : 0.030248533934354782\n",
      "\n",
      "\u001b[1;37mBatch loss 294 : 0.09674085676670074\n",
      "\n",
      "\u001b[1;37mBatch loss 295 : 0.0128971291705966\n",
      "\n",
      "\u001b[1;37mBatch loss 296 : 0.033674612641334534\n",
      "\n",
      "\u001b[1;37mBatch loss 297 : 0.04098382592201233\n",
      "\n",
      "\u001b[1;37mBatch loss 298 : 0.05077999457716942\n",
      "\n",
      "\u001b[1;37mBatch loss 299 : 0.031676243990659714\n",
      "\n",
      "\u001b[1;37mBatch loss 300 : 0.03709768131375313\n",
      "\n",
      "\u001b[1;37mBatch loss 301 : 0.06599389016628265\n",
      "\n",
      "\u001b[1;37mBatch loss 302 : 0.04147827997803688\n",
      "\n",
      "\u001b[1;37mBatch loss 303 : 0.0683256983757019\n",
      "\n",
      "\u001b[1;37mBatch loss 304 : 0.08779625594615936\n",
      "\n",
      "\u001b[1;37mBatch loss 305 : 0.028799638152122498\n",
      "\n",
      "\u001b[1;37mBatch loss 306 : 0.020183974876999855\n",
      "\n",
      "\u001b[1;37mBatch loss 307 : 0.08440125733613968\n",
      "\n",
      "\u001b[1;37mBatch loss 308 : 0.026981087401509285\n",
      "\n",
      "\u001b[1;37mBatch loss 309 : 0.03657317906618118\n",
      "\n",
      "\u001b[1;37mBatch loss 310 : 0.07454609870910645\n",
      "\n",
      "\u001b[1;37mBatch loss 311 : 0.032202452421188354\n",
      "\n",
      "\u001b[1;37mBatch loss 312 : 0.045202117413282394\n",
      "\n",
      "\u001b[1;37mBatch loss 313 : 0.06665635854005814\n",
      "\n",
      "\u001b[1;37mBatch loss 314 : 0.043013062328100204\n",
      "\n",
      "\u001b[1;37mBatch loss 315 : 0.043532222509384155\n",
      "\n",
      "\u001b[1;37mBatch loss 316 : 0.08483818918466568\n",
      "\n",
      "\u001b[1;37mBatch loss 317 : 0.035065822303295135\n",
      "\n",
      "\u001b[1;37mBatch loss 318 : 0.040078304708004\n",
      "\n",
      "\u001b[1;37mBatch loss 319 : 0.029486829414963722\n",
      "\n",
      "\u001b[1;37mBatch loss 320 : 0.04715808480978012\n",
      "\n",
      "\u001b[1;37mBatch loss 321 : 0.03150649741292\n",
      "\n",
      "\u001b[1;37mBatch loss 322 : 0.09696056693792343\n",
      "\n",
      "\u001b[1;37mBatch loss 323 : 0.3167050778865814\n",
      "\n",
      "\u001b[1;37mBatch loss 324 : 0.09091714769601822\n",
      "\n",
      "\u001b[1;37mBatch loss 325 : 0.04937608540058136\n",
      "\n",
      "\u001b[1;37mBatch loss 326 : 0.10705802589654922\n",
      "\n",
      "\u001b[1;37mBatch loss 327 : 0.021806377917528152\n",
      "\n",
      "\u001b[1;37mBatch loss 328 : 0.08230032026767731\n",
      "\n",
      "\u001b[1;37mBatch loss 329 : 0.06934456527233124\n",
      "\n",
      "\u001b[1;37mBatch loss 330 : 0.015306837856769562\n",
      "\n",
      "\u001b[1;37mBatch loss 331 : 0.09140190482139587\n",
      "\n",
      "\u001b[1;37mBatch loss 332 : 0.050119705498218536\n",
      "\n",
      "\u001b[1;37mBatch loss 333 : 0.06075110286474228\n",
      "\n",
      "\u001b[1;37mBatch loss 334 : 0.04836798831820488\n",
      "\n",
      "\u001b[1;37mBatch loss 335 : 0.02823285013437271\n",
      "\n",
      "\u001b[1;37mBatch loss 336 : 0.05818234011530876\n",
      "\n",
      "\u001b[1;37mBatch loss 337 : 0.05031950771808624\n",
      "\n",
      "\u001b[1;37mBatch loss 338 : 0.03713488206267357\n",
      "\n",
      "\u001b[1;37mBatch loss 339 : 0.03339623287320137\n",
      "\n",
      "\u001b[1;37mBatch loss 340 : 0.04957342520356178\n",
      "\n",
      "\u001b[1;37mBatch loss 341 : 0.04287588968873024\n",
      "\n",
      "\u001b[1;37mBatch loss 342 : 0.02790025994181633\n",
      "\n",
      "\u001b[1;37mBatch loss 343 : 0.028417179360985756\n",
      "\n",
      "\u001b[1;37mBatch loss 344 : 0.08525082468986511\n",
      "\n",
      "\u001b[1;37mBatch loss 345 : 0.09461592882871628\n",
      "\n",
      "\u001b[1;37mBatch loss 346 : 0.04494952782988548\n",
      "\n",
      "\u001b[1;37mBatch loss 347 : 0.025196602568030357\n",
      "\n",
      "\u001b[1;37mBatch loss 348 : 0.06487193703651428\n",
      "\n",
      "\u001b[1;37mBatch loss 349 : 0.052732717245817184\n",
      "\n",
      "\u001b[1;37mBatch loss 350 : 0.08974013477563858\n",
      "\n",
      "\u001b[1;37mBatch loss 351 : 0.0321987085044384\n",
      "\n",
      "\u001b[1;37mBatch loss 352 : 0.07115820795297623\n",
      "\n",
      "\u001b[1;37mBatch loss 353 : 0.03254510834813118\n",
      "\n",
      "\u001b[1;37mBatch loss 354 : 0.031523287296295166\n",
      "\n",
      "\u001b[1;37mBatch loss 355 : 0.03720104321837425\n",
      "\n",
      "\u001b[1;37mBatch loss 356 : 0.01677991822361946\n",
      "\n",
      "\u001b[1;37mBatch loss 357 : 0.08428553491830826\n",
      "\n",
      "\u001b[1;37mBatch loss 358 : 0.036703478544950485\n",
      "\n",
      "\u001b[1;37mBatch loss 359 : 0.10497049987316132\n",
      "\n",
      "\u001b[1;37mBatch loss 360 : 0.05826762318611145\n",
      "\n",
      "\u001b[1;37mBatch loss 361 : 0.05939111113548279\n",
      "\n",
      "\u001b[1;37mBatch loss 362 : 0.04508451372385025\n",
      "\n",
      "\u001b[1;37mBatch loss 363 : 0.027768077328801155\n",
      "\n",
      "\u001b[1;37mBatch loss 364 : 0.04158616438508034\n",
      "\n",
      "\u001b[1;37mBatch loss 365 : 0.06546038389205933\n",
      "\n",
      "\u001b[1;37mBatch loss 366 : 0.025711093097925186\n",
      "\n",
      "\u001b[1;37mBatch loss 367 : 0.08570458739995956\n",
      "\n",
      "\u001b[1;37mBatch loss 368 : 0.02729739621281624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;37mBatch loss 369 : 0.030529938638210297\n",
      "\n",
      "\u001b[1;37mBatch loss 370 : 0.03899368271231651\n",
      "\n",
      "\u001b[1;37mBatch loss 371 : 0.04775836318731308\n",
      "\n",
      "\u001b[1;37mBatch loss 372 : 0.06384135037660599\n",
      "\n",
      "\u001b[1;37mBatch loss 373 : 0.050309695303440094\n",
      "\n",
      "\u001b[1;37mBatch loss 374 : 0.055152975022792816\n",
      "\n",
      "\u001b[1;37mBatch loss 375 : 0.06687694787979126\n",
      "\n",
      "\u001b[1;37mBatch loss 376 : 0.03596597909927368\n",
      "\n",
      "\u001b[1;37mBatch loss 377 : 0.03613467141985893\n",
      "\n",
      "\u001b[1;37mBatch loss 378 : 0.049278534948825836\n",
      "\n",
      "\u001b[1;37mBatch loss 379 : 0.04847557470202446\n",
      "\n",
      "\u001b[1;37mBatch loss 380 : 0.061841823160648346\n",
      "\n",
      "\u001b[1;37mBatch loss 381 : 0.04260622337460518\n",
      "\n",
      "\u001b[1;37mBatch loss 382 : 0.09594631940126419\n",
      "\n",
      "\u001b[1;37mBatch loss 383 : 0.044280119240283966\n",
      "\n",
      "\u001b[1;37mBatch loss 384 : 0.050975095480680466\n",
      "\n",
      "\u001b[1;37mBatch loss 385 : 0.022749679163098335\n",
      "\n",
      "\u001b[1;37mBatch loss 386 : 0.06224841624498367\n",
      "\n",
      "\u001b[1;37mBatch loss 387 : 0.07056812196969986\n",
      "\n",
      "\u001b[1;37mBatch loss 388 : 0.06944659352302551\n",
      "\n",
      "\u001b[1;37mBatch loss 389 : 0.0303342342376709\n",
      "\n",
      "\u001b[1;37mBatch loss 390 : 0.04486014321446419\n",
      "\n",
      "\u001b[1;37mBatch loss 391 : 0.06734660267829895\n",
      "\n",
      "\u001b[1;37mBatch loss 392 : 0.03287290409207344\n",
      "\n",
      "\u001b[1;37mBatch loss 393 : 0.02634364552795887\n",
      "\n",
      "\u001b[1;37mBatch loss 394 : 0.04365306347608566\n",
      "\n",
      "\u001b[1;37mBatch loss 395 : 0.0166530292481184\n",
      "\n",
      "\u001b[1;37mBatch loss 396 : 0.048893123865127563\n",
      "\n",
      "\u001b[1;37mBatch loss 397 : 0.03360239416360855\n",
      "\n",
      "\u001b[1;37mBatch loss 398 : 0.023416580632328987\n",
      "\n",
      "\u001b[1;37mBatch loss 399 : 0.027771448716521263\n",
      "\n",
      "\u001b[1;37mBatch loss 400 : 0.03417407348752022\n",
      "\n",
      "\u001b[1;37mBatch loss 401 : 0.04486637935042381\n",
      "\n",
      "\u001b[1;37mBatch loss 402 : 0.06625977903604507\n",
      "\n",
      "\u001b[1;37mBatch loss 403 : 0.10234418511390686\n",
      "\n",
      "\u001b[1;37mBatch loss 404 : 0.08416280150413513\n",
      "\n",
      "\u001b[1;37mBatch loss 405 : 0.07497193664312363\n",
      "\n",
      "\u001b[1;37mBatch loss 406 : 0.06816446781158447\n",
      "\n",
      "\u001b[1;37mBatch loss 407 : 0.026591693982481956\n",
      "\n",
      "\u001b[1;37mBatch loss 408 : 0.05309921130537987\n",
      "\n",
      "\u001b[1;37mBatch loss 409 : 0.07484441995620728\n",
      "\n",
      "\u001b[1;37mBatch loss 410 : 0.049267563968896866\n",
      "\n",
      "\u001b[1;37mBatch loss 411 : 0.046622440218925476\n",
      "\n",
      "\u001b[1;37mBatch loss 412 : 0.018091600388288498\n",
      "\n",
      "\u001b[1;37mBatch loss 413 : 0.042190879583358765\n",
      "\n",
      "\u001b[1;37mBatch loss 414 : 0.04700709506869316\n",
      "\n",
      "\u001b[1;37mBatch loss 415 : 0.03999367356300354\n",
      "\n",
      "\u001b[1;37mBatch loss 416 : 0.09458532929420471\n",
      "\n",
      "\u001b[1;37mBatch loss 417 : 0.05519122630357742\n",
      "\n",
      "\u001b[1;37mBatch loss 418 : 0.032088398933410645\n",
      "\n",
      "\u001b[1;37mBatch loss 419 : 0.03298744186758995\n",
      "\n",
      "\u001b[1;37mBatch loss 420 : 0.047273267060518265\n",
      "\n",
      "\u001b[1;37mBatch loss 421 : 0.03501421958208084\n",
      "\n",
      "\u001b[1;37mBatch loss 422 : 0.09221301227807999\n",
      "\n",
      "\u001b[1;37mBatch loss 423 : 0.08365300297737122\n",
      "\n",
      "\u001b[1;37mBatch loss 424 : 0.015184891410171986\n",
      "\n",
      "\u001b[1;37mBatch loss 425 : 0.0512104332447052\n",
      "\n",
      "\u001b[1;37mBatch loss 426 : 0.05234144628047943\n",
      "\n",
      "\u001b[1;37mBatch loss 427 : 0.039934925734996796\n",
      "\n",
      "\u001b[1;37mBatch loss 428 : 0.033196769654750824\n",
      "\n",
      "\u001b[1;37mBatch loss 429 : 0.020421018823981285\n",
      "\n",
      "\u001b[1;37mBatch loss 430 : 0.04689749330282211\n",
      "\n",
      "\u001b[1;37mBatch loss 431 : 0.06105981394648552\n",
      "\n",
      "\u001b[1;37mBatch loss 432 : 0.06575261801481247\n",
      "\n",
      "\u001b[1;37mBatch loss 433 : 0.019387148320674896\n",
      "\n",
      "\u001b[1;37mBatch loss 434 : 0.037503186613321304\n",
      "\n",
      "\u001b[1;37mBatch loss 435 : 0.028804495930671692\n",
      "\n",
      "\u001b[1;37mBatch loss 436 : 0.048445601016283035\n",
      "\n",
      "\u001b[1;37mBatch loss 437 : 0.057165514677762985\n",
      "\n",
      "\u001b[1;37mBatch loss 438 : 0.02322988212108612\n",
      "\n",
      "\u001b[1;37mBatch loss 439 : 0.05663151293992996\n",
      "\n",
      "\u001b[1;37mBatch loss 440 : 0.04512440040707588\n",
      "\n",
      "\u001b[1;37mBatch loss 441 : 0.030500205233693123\n",
      "\n",
      "\u001b[1;37mBatch loss 442 : 0.03316965699195862\n",
      "\n",
      "\u001b[1;37mBatch loss 443 : 0.09718140959739685\n",
      "\n",
      "\u001b[1;37mBatch loss 444 : 0.07559987902641296\n",
      "\n",
      "\u001b[1;37mBatch loss 445 : 0.02584133669734001\n",
      "\n",
      "\u001b[1;37mBatch loss 446 : 0.03349896892905235\n",
      "\n",
      "\u001b[1;37mBatch loss 447 : 0.024632498621940613\n",
      "\n",
      "\u001b[1;37mBatch loss 448 : 0.037813082337379456\n",
      "\n",
      "\u001b[1;37mBatch loss 449 : 0.05231967568397522\n",
      "\n",
      "\u001b[1;37mBatch loss 450 : 0.03384307771921158\n",
      "\n",
      "\u001b[1;37mBatch loss 451 : 0.04402538388967514\n",
      "\n",
      "\u001b[1;37mBatch loss 452 : 0.047768134623765945\n",
      "\n",
      "\u001b[1;37mBatch loss 453 : 0.031622372567653656\n",
      "\n",
      "\u001b[1;37mBatch loss 454 : 0.0364801362156868\n",
      "\n",
      "\u001b[1;37mBatch loss 455 : 0.04903875291347504\n",
      "\n",
      "\u001b[1;37mBatch loss 456 : 0.048482079058885574\n",
      "\n",
      "\u001b[1;37mBatch loss 457 : 0.03824019059538841\n",
      "\n",
      "\u001b[1;37mBatch loss 458 : 0.04484456405043602\n",
      "\n",
      "\u001b[1;37mBatch loss 459 : 0.03134572505950928\n",
      "\n",
      "\u001b[1;37mBatch loss 460 : 0.028485091403126717\n",
      "\n",
      "\u001b[1;37mBatch loss 461 : 0.04599521681666374\n",
      "\n",
      "\u001b[1;37mBatch loss 462 : 0.026751264929771423\n",
      "\n",
      "\u001b[1;37mBatch loss 463 : 0.0278923399746418\n",
      "\n",
      "\u001b[1;37mBatch loss 464 : 0.048845142126083374\n",
      "\n",
      "\u001b[1;37mBatch loss 465 : 0.05598333850502968\n",
      "\n",
      "\u001b[1;37mBatch loss 466 : 0.059747520834207535\n",
      "\n",
      "\u001b[1;37mBatch loss 467 : 0.04521035775542259\n",
      "\n",
      "\u001b[1;37mBatch loss 468 : 0.03604859113693237\n",
      "\n",
      "\u001b[1;37mBatch loss 469 : 0.05783836543560028\n",
      "\n",
      "\u001b[1;37mBatch loss 470 : 0.06186354532837868\n",
      "\n",
      "\u001b[1;37mBatch loss 471 : 0.08024716377258301\n",
      "\n",
      "\u001b[1;37mBatch loss 472 : 0.017783617600798607\n",
      "\n",
      "\u001b[1;37mBatch loss 473 : 0.0998934805393219\n",
      "\n",
      "\u001b[1;37mBatch loss 474 : 0.09113157540559769\n",
      "\n",
      "\u001b[1;37mBatch loss 475 : 0.049048248678445816\n",
      "\n",
      "\u001b[1;37mBatch loss 476 : 0.04005908593535423\n",
      "\n",
      "\u001b[1;37mBatch loss 477 : 0.04878462478518486\n",
      "\n",
      "\u001b[1;37mBatch loss 478 : 0.03564345836639404\n",
      "\n",
      "\u001b[1;37mBatch loss 479 : 0.026916442438960075\n",
      "\n",
      "\u001b[1;37mBatch loss 480 : 0.05271280184388161\n",
      "\n",
      "\u001b[1;37mBatch loss 481 : 0.026736004278063774\n",
      "\n",
      "\u001b[1;37mBatch loss 482 : 0.02858145721256733\n",
      "\n",
      "\u001b[1;37mBatch loss 483 : 0.06721972674131393\n",
      "\n",
      "\u001b[1;37mBatch loss 484 : 0.07768228650093079\n",
      "\n",
      "\u001b[1;37mBatch loss 485 : 0.04513602703809738\n",
      "\n",
      "\u001b[1;37mBatch loss 486 : 0.044426560401916504\n",
      "\n",
      "\u001b[1;37mBatch loss 487 : 0.038776692003011703\n",
      "\n",
      "\u001b[1;37mBatch loss 488 : 0.03168056160211563\n",
      "\n",
      "\u001b[1;37mBatch loss 489 : 0.03381507098674774\n",
      "\n",
      "\u001b[1;37mBatch loss 490 : 0.03388515114784241\n",
      "\n",
      "\u001b[1;37mBatch loss 491 : 0.057519689202308655\n",
      "\n",
      "\u001b[1;37mBatch loss 492 : 0.09134254604578018\n",
      "\n",
      "\u001b[1;37mBatch loss 493 : 0.050716813653707504\n",
      "\n",
      "\u001b[1;37mBatch loss 494 : 0.05392369627952576\n",
      "\n",
      "\u001b[1;37mBatch loss 495 : 0.043698735535144806\n",
      "\n",
      "\u001b[1;37mBatch loss 496 : 0.03483407199382782\n",
      "\n",
      "\u001b[1;37mBatch loss 497 : 0.0025454480201005936\n",
      "\n",
      "\u001b[1;37mBatch loss 498 : 0.053506966680288315\n",
      "\n",
      "\u001b[1;37mBatch loss 499 : 0.04230847954750061\n",
      "\n",
      "\u001b[1;37mBatch loss 500 : 0.042418427765369415\n",
      "\n",
      "\u001b[1;37mBatch loss 501 : 0.05655136704444885\n",
      "\n",
      "\u001b[1;37mBatch loss 502 : 0.039860621094703674\n",
      "\n",
      "\u001b[1;37mBatch loss 503 : 0.023357562720775604\n",
      "\n",
      "\u001b[1;37mBatch loss 504 : 0.0466034896671772\n",
      "\n",
      "\u001b[1;37mBatch loss 505 : 0.05333723500370979\n",
      "\n",
      "\u001b[1;37mBatch loss 506 : 0.047362543642520905\n",
      "\n",
      "\u001b[1;37mBatch loss 507 : 0.02986549586057663\n",
      "\n",
      "\u001b[1;37mBatch loss 508 : 0.058382436633110046\n",
      "\n",
      "\u001b[1;37mBatch loss 509 : 0.05537300929427147\n",
      "\n",
      "\u001b[1;37mBatch loss 510 : 0.05881057679653168\n",
      "\n",
      "\u001b[1;37mBatch loss 511 : 0.019488809630274773\n",
      "\n",
      "\u001b[1;37mBatch loss 512 : 0.06850335001945496\n",
      "\n",
      "\u001b[1;37mBatch loss 513 : 0.04308056831359863\n",
      "\n",
      "\u001b[1;37mBatch loss 514 : 0.1127748191356659\n",
      "\n",
      "\u001b[1;37mBatch loss 515 : 0.023045729845762253\n",
      "\n",
      "\u001b[1;37mBatch loss 516 : 0.07278256118297577\n",
      "\n",
      "\u001b[1;37mBatch loss 517 : 0.04857045039534569\n",
      "\n",
      "\u001b[1;37mBatch loss 518 : 0.025936413556337357\n",
      "\n",
      "\u001b[1;37mBatch loss 519 : 0.07030344754457474\n",
      "\n",
      "\u001b[1;37mBatch loss 520 : 0.03635344281792641\n",
      "\n",
      "\u001b[1;37mBatch loss 521 : 0.0642172172665596\n",
      "\n",
      "\u001b[1;37mBatch loss 522 : 0.08212899416685104\n",
      "\n",
      "\u001b[1;37mBatch loss 523 : 0.0439569428563118\n",
      "\n",
      "\u001b[1;37mBatch loss 524 : 0.03373120725154877\n",
      "\n",
      "\u001b[1;37mBatch loss 525 : 0.038250502198934555\n",
      "\n",
      "\u001b[1;37mBatch loss 526 : 0.07046815007925034\n",
      "\n",
      "\u001b[1;37mBatch loss 527 : 0.020014269277453423\n",
      "\n",
      "\u001b[1;37mBatch loss 528 : 0.08124832808971405\n",
      "\n",
      "\u001b[1;37mBatch loss 529 : 0.05241464078426361\n",
      "\n",
      "\u001b[1;37mBatch loss 530 : 0.030033918097615242\n",
      "\n",
      "\u001b[1;37mBatch loss 531 : 0.03922257572412491\n",
      "\n",
      "\u001b[1;37mBatch loss 532 : 0.04235134646296501\n",
      "\n",
      "\u001b[1;37mBatch loss 533 : 0.027213193476200104\n",
      "\n",
      "\u001b[1;37mBatch loss 534 : 0.02437773533165455\n",
      "\n",
      "\u001b[1;37mBatch loss 535 : 0.01390518806874752\n",
      "\n",
      "\u001b[1;37mBatch loss 536 : 0.02300826832652092\n",
      "\n",
      "\u001b[1;37mBatch loss 537 : 0.029397759586572647\n",
      "\n",
      "\u001b[1;37mBatch loss 538 : 0.05989248678088188\n",
      "\n",
      "\u001b[1;37mBatch loss 539 : 0.04836016520857811\n",
      "\n",
      "\u001b[1;37mBatch loss 540 : 0.04349730908870697\n",
      "\n",
      "\u001b[1;37mBatch loss 541 : 0.03681235387921333\n",
      "\n",
      "\u001b[1;37mBatch loss 542 : 0.1021440401673317\n",
      "\n",
      "\u001b[1;37mBatch loss 543 : 0.03158838301897049\n",
      "\n",
      "\u001b[1;37mBatch loss 544 : 0.04072221368551254\n",
      "\n",
      "\u001b[1;37mBatch loss 545 : 0.027737176045775414\n",
      "\n",
      "\u001b[1;37mBatch loss 546 : 0.0277171079069376\n",
      "\n",
      "\u001b[1;37mBatch loss 547 : 0.038143403828144073\n",
      "\n",
      "\u001b[1;37mBatch loss 548 : 0.010600009001791477\n",
      "\n",
      "\u001b[1;37mBatch loss 549 : 0.050323694944381714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;37mBatch loss 550 : 0.012240168638527393\n",
      "\n",
      "\u001b[1;37mBatch loss 551 : 0.06540932506322861\n",
      "\n",
      "\u001b[1;37mBatch loss 552 : 0.025776639580726624\n",
      "\n",
      "\u001b[1;37mBatch loss 553 : 0.03813769295811653\n",
      "\n",
      "\u001b[1;37mBatch loss 554 : 0.017622720450162888\n",
      "\n",
      "\u001b[1;37mBatch loss 555 : 0.06668172776699066\n",
      "\n",
      "\u001b[1;37mBatch loss 556 : 0.02338511496782303\n",
      "\n",
      "\u001b[1;37mBatch loss 557 : 0.04723621904850006\n",
      "\n",
      "\u001b[1;37mBatch loss 558 : 0.04028474912047386\n",
      "\n",
      "\u001b[1;37mBatch loss 559 : 0.03333281725645065\n",
      "\n",
      "\u001b[1;37mBatch loss 560 : 0.0349687896668911\n",
      "\n",
      "\u001b[1;37mBatch loss 561 : 0.03669969365000725\n",
      "\n",
      "\u001b[1;37mBatch loss 562 : 0.03190365061163902\n",
      "\n",
      "\u001b[1;37mBatch loss 563 : 0.032557740807533264\n",
      "\n",
      "\u001b[1;37mBatch loss 564 : 0.018017416819930077\n",
      "\n",
      "\u001b[1;37mBatch loss 565 : 0.03861093893647194\n",
      "\n",
      "\u001b[1;37mBatch loss 566 : 0.045645929872989655\n",
      "\n",
      "\u001b[1;37mBatch loss 567 : 0.05149345099925995\n",
      "\n",
      "\u001b[1;37mBatch loss 568 : 0.038020577281713486\n",
      "\n",
      "\u001b[1;37mBatch loss 569 : 0.02989261783659458\n",
      "\n",
      "\u001b[1;37mBatch loss 570 : 0.013011613860726357\n",
      "\n",
      "\u001b[1;37mBatch loss 571 : 0.0485173799097538\n",
      "\n",
      "\u001b[1;37mBatch loss 572 : 0.04347303509712219\n",
      "\n",
      "\u001b[1;37mBatch loss 573 : 0.057829681783914566\n",
      "\n",
      "\u001b[1;37mBatch loss 574 : 0.035002779215574265\n",
      "\n",
      "\u001b[1;37mBatch loss 575 : 0.03855031356215477\n",
      "\n",
      "\u001b[1;37mBatch loss 576 : 0.046278215944767\n",
      "\n",
      "\u001b[1;37mBatch loss 577 : 0.04908798262476921\n",
      "\n",
      "\u001b[1;37mBatch loss 578 : 0.039730824530124664\n",
      "\n",
      "\u001b[1;37mBatch loss 579 : 0.02628408744931221\n",
      "\n",
      "\u001b[1;37mBatch loss 580 : 0.02672920934855938\n",
      "\n",
      "\u001b[1;37mBatch loss 581 : 0.059324465692043304\n",
      "\n",
      "\u001b[1;37mBatch loss 582 : 0.04642864316701889\n",
      "\n",
      "\u001b[1;37mBatch loss 583 : 0.02510085329413414\n",
      "\n",
      "\u001b[1;37mBatch loss 584 : 0.0726371556520462\n",
      "\n",
      "\u001b[1;37mBatch loss 585 : 0.028429651632905006\n",
      "\n",
      "\u001b[1;37mBatch loss 586 : 0.059379421174526215\n",
      "\n",
      "\u001b[1;37mBatch loss 587 : 0.03211025148630142\n",
      "\n",
      "\u001b[1;37mBatch loss 588 : 0.018124472349882126\n",
      "\n",
      "\u001b[1;37mBatch loss 589 : 0.02629253827035427\n",
      "\n",
      "\u001b[1;37mBatch loss 590 : 0.029000088572502136\n",
      "\n",
      "\u001b[1;37mBatch loss 591 : 0.06425563246011734\n",
      "\n",
      "\u001b[1;37mBatch loss 592 : 0.10553660988807678\n",
      "\n",
      "\u001b[1;37mBatch loss 593 : 0.019100455567240715\n",
      "\n",
      "\u001b[1;37mBatch loss 594 : 0.059742461889982224\n",
      "\n",
      "\u001b[1;37mBatch loss 595 : 0.026953671127557755\n",
      "\n",
      "\u001b[1;37mBatch loss 596 : 0.050313182175159454\n",
      "\n",
      "\u001b[1;37mBatch loss 597 : 0.044245894998311996\n",
      "\n",
      "\u001b[1;37mBatch loss 598 : 0.04127831757068634\n",
      "\n",
      "\u001b[1;37mBatch loss 599 : 0.057047825306653976\n",
      "\n",
      "\u001b[1;37mBatch loss 600 : 0.07962574809789658\n",
      "\n",
      "\u001b[1;37mBatch loss 601 : 0.022476710379123688\n",
      "\n",
      "\u001b[1;37mBatch loss 602 : 0.02835077978670597\n",
      "\n",
      "\u001b[1;37mBatch loss 603 : 0.0216500423848629\n",
      "\n",
      "\u001b[1;37mBatch loss 604 : 0.058826617896556854\n",
      "\n",
      "\u001b[1;37mBatch loss 605 : 0.03507920727133751\n",
      "\n",
      "\u001b[1;37mBatch loss 606 : 0.021080773323774338\n",
      "\n",
      "\u001b[1;37mBatch loss 607 : 0.0347883440554142\n",
      "\n",
      "\u001b[1;37mBatch loss 608 : 0.03306489810347557\n",
      "\n",
      "\u001b[1;37mBatch loss 609 : 0.019540896639227867\n",
      "\n",
      "\u001b[1;37mBatch loss 610 : 0.02458290010690689\n",
      "\n",
      "\u001b[1;37mBatch loss 611 : 0.05626760050654411\n",
      "\n",
      "\u001b[1;37mBatch loss 612 : 0.04235531762242317\n",
      "\n",
      "\u001b[1;37mBatch loss 613 : 0.03472452238202095\n",
      "\n",
      "\u001b[1;37mBatch loss 614 : 0.03744606673717499\n",
      "\n",
      "\u001b[1;37mBatch loss 615 : 0.0679292306303978\n",
      "\n",
      "\u001b[1;37mBatch loss 616 : 0.019395077601075172\n",
      "\n",
      "\u001b[1;37mBatch loss 617 : 0.03770012408494949\n",
      "\n",
      "\u001b[1;37mBatch loss 618 : 0.06773415952920914\n",
      "\n",
      "\u001b[1;37mBatch loss 619 : 0.03547775372862816\n",
      "\n",
      "\u001b[1;37mBatch loss 620 : 0.03142588585615158\n",
      "\n",
      "\u001b[1;37mBatch loss 621 : 0.045918501913547516\n",
      "\n",
      "\u001b[1;37mBatch loss 622 : 0.028719928115606308\n",
      "\n",
      "\u001b[1;37mBatch loss 623 : 0.04547156020998955\n",
      "\n",
      "\u001b[1;37mBatch loss 624 : 0.042349230498075485\n",
      "\n",
      "\u001b[1;37mBatch loss 625 : 0.04592243582010269\n",
      "\n",
      "\u001b[1;37mBatch loss 626 : 0.05580638721585274\n",
      "\n",
      "\u001b[1;37mBatch loss 627 : 0.04012375324964523\n",
      "\n",
      "\u001b[1;37mBatch loss 628 : 0.03856448084115982\n",
      "\n",
      "\u001b[1;37mBatch loss 629 : 0.041483934968709946\n",
      "\n",
      "\u001b[1;37mBatch loss 630 : 0.04865580052137375\n",
      "\n",
      "\u001b[1;37mBatch loss 631 : 0.039248768240213394\n",
      "\n",
      "\u001b[1;37mBatch loss 632 : 0.021098360419273376\n",
      "\n",
      "\u001b[1;37mBatch loss 633 : 0.06476422399282455\n",
      "\n",
      "\u001b[1;37mBatch loss 634 : 0.03818928077816963\n",
      "\n",
      "\u001b[1;37mBatch loss 635 : 0.053625211119651794\n",
      "\n",
      "\u001b[1;37mBatch loss 636 : 0.05371224880218506\n",
      "\n",
      "\u001b[1;37mBatch loss 637 : 0.029915397986769676\n",
      "\n",
      "\u001b[1;37mBatch loss 638 : 0.035456374287605286\n",
      "\n",
      "\u001b[1;37mBatch loss 639 : 0.022411959245800972\n",
      "\n",
      "\u001b[1;37mBatch loss 640 : 0.026993276551365852\n",
      "\n",
      "\u001b[1;37mBatch loss 641 : 0.04400080814957619\n",
      "\n",
      "\u001b[1;37mBatch loss 642 : 0.043323203921318054\n",
      "\n",
      "\u001b[1;37mBatch loss 643 : 0.024702072143554688\n",
      "\n",
      "\u001b[1;37mBatch loss 644 : 0.05518992990255356\n",
      "\n",
      "\u001b[1;37mBatch loss 645 : 0.02476714365184307\n",
      "\n",
      "\u001b[1;37mBatch loss 646 : 0.03676377609372139\n",
      "\n",
      "\u001b[1;37mBatch loss 647 : 0.055933088064193726\n",
      "\n",
      "\u001b[1;37mBatch loss 648 : 0.03670283779501915\n",
      "\n",
      "\u001b[1;37mBatch loss 649 : 0.03612707927823067\n",
      "\n",
      "\u001b[1;37mBatch loss 650 : 0.015978701412677765\n",
      "\n",
      "\u001b[1;37mBatch loss 651 : 0.019026651978492737\n",
      "\n",
      "\u001b[1;37mBatch loss 652 : 0.04150349646806717\n",
      "\n",
      "\u001b[1;37mBatch loss 653 : 0.0633930042386055\n",
      "\n",
      "\u001b[1;37mBatch loss 654 : 0.021430604159832\n",
      "\n",
      "\u001b[1;37mBatch loss 655 : 0.07036122679710388\n",
      "\n",
      "\u001b[1;37mBatch loss 656 : 0.010566303506493568\n",
      "\n",
      "\u001b[1;37mBatch loss 657 : 0.0370011143386364\n",
      "\n",
      "\u001b[1;37mBatch loss 658 : 0.04621635377407074\n",
      "\n",
      "\u001b[1;37mBatch loss 659 : 0.06532298773527145\n",
      "\n",
      "\u001b[1;37mBatch loss 660 : 0.0458553209900856\n",
      "\n",
      "\u001b[1;37mBatch loss 661 : 0.07017570734024048\n",
      "\n",
      "\u001b[1;37mBatch loss 662 : 0.01195551734417677\n",
      "\n",
      "\u001b[1;37mBatch loss 663 : 0.03931170701980591\n",
      "\n",
      "\u001b[1;37mBatch loss 664 : 0.017758557572960854\n",
      "\n",
      "\u001b[1;37mBatch loss 665 : 0.06123980134725571\n",
      "\n",
      "\u001b[1;37mBatch loss 666 : 0.022437788546085358\n",
      "\n",
      "\u001b[1;37mBatch loss 667 : 0.01089464034885168\n",
      "\n",
      "\u001b[1;37mBatch loss 668 : 0.018875397741794586\n",
      "\n",
      "\u001b[1;37mBatch loss 669 : 0.020349465310573578\n",
      "\n",
      "\u001b[1;37mBatch loss 670 : 0.01289040595293045\n",
      "\n",
      "\u001b[1;37mBatch loss 671 : 0.09512975066900253\n",
      "\n",
      "\u001b[1;37mBatch loss 672 : 0.029278023168444633\n",
      "\n",
      "\u001b[1;37mBatch loss 673 : 0.07033929973840714\n",
      "\n",
      "\u001b[1;37mBatch loss 674 : 0.05889806151390076\n",
      "\n",
      "\u001b[1;37mBatch loss 675 : 0.02221321500837803\n",
      "\n",
      "\u001b[1;37mBatch loss 676 : 0.05821641907095909\n",
      "\n",
      "\u001b[1;37mBatch loss 677 : 0.03243861347436905\n",
      "\n",
      "\u001b[1;37mBatch loss 678 : 0.07830415666103363\n",
      "\n",
      "\u001b[1;37mBatch loss 679 : 0.01676713488996029\n",
      "\n",
      "\u001b[1;37mBatch loss 680 : 0.03437401354312897\n",
      "\n",
      "\u001b[1;37mBatch loss 681 : 0.06523877382278442\n",
      "\n",
      "\u001b[1;37mBatch loss 682 : 0.017200326547026634\n",
      "\n",
      "\u001b[1;37mBatch loss 683 : 0.043803416192531586\n",
      "\n",
      "\u001b[1;37mBatch loss 684 : 0.018163496628403664\n",
      "\n",
      "\u001b[1;37mBatch loss 685 : 0.029989900067448616\n",
      "\n",
      "\u001b[1;37mBatch loss 686 : 0.05679912865161896\n",
      "\n",
      "\u001b[1;37mBatch loss 687 : 0.013521039858460426\n",
      "\n",
      "\u001b[1;37mBatch loss 688 : 0.03409890457987785\n",
      "\n",
      "\u001b[1;37mBatch loss 689 : 0.02826128900051117\n",
      "\n",
      "\u001b[1;37mBatch loss 690 : 0.05497077479958534\n",
      "\n",
      "\u001b[1;37mBatch loss 691 : 0.042716074734926224\n",
      "\n",
      "\u001b[1;37mBatch loss 692 : 0.044576700776815414\n",
      "\n",
      "\u001b[1;37mBatch loss 693 : 0.020160382613539696\n",
      "\n",
      "\u001b[1;37mBatch loss 694 : 0.04254535213112831\n",
      "\n",
      "\u001b[1;37mBatch loss 695 : 0.05080718919634819\n",
      "\n",
      "\u001b[1;37mBatch loss 696 : 0.05615023523569107\n",
      "\n",
      "\u001b[1;37mBatch loss 697 : 0.030443422496318817\n",
      "\n",
      "\u001b[1;37mBatch loss 698 : 0.054479703307151794\n",
      "\n",
      "\u001b[1;37mBatch loss 699 : 0.025888415053486824\n",
      "\n",
      "\u001b[1;37mBatch loss 700 : 0.023732243105769157\n",
      "\n",
      "\u001b[1;37mBatch loss 701 : 0.006864314898848534\n",
      "\n",
      "\u001b[1;37mBatch loss 702 : 0.03990744426846504\n",
      "\n",
      "\u001b[1;37mBatch loss 703 : 0.03388623148202896\n",
      "\n",
      "\u001b[1;37mBatch loss 704 : 0.04075158014893532\n",
      "\n",
      "\u001b[1;37mBatch loss 705 : 0.04730520769953728\n",
      "\n",
      "\u001b[1;37mBatch loss 706 : 0.01939639262855053\n",
      "\n",
      "\u001b[1;37mBatch loss 707 : 0.05442728102207184\n",
      "\n",
      "\u001b[1;37mBatch loss 708 : 0.014948166906833649\n",
      "\n",
      "\u001b[1;37mBatch loss 709 : 0.017442548647522926\n",
      "\n",
      "\u001b[1;37mBatch loss 710 : 0.02326131798326969\n",
      "\n",
      "\u001b[1;37mBatch loss 711 : 0.023919453844428062\n",
      "\n",
      "\u001b[1;37mBatch loss 712 : 0.03363844379782677\n",
      "\n",
      "\u001b[1;37mBatch loss 713 : 0.06795041263103485\n",
      "\n",
      "\u001b[1;37mBatch loss 714 : 0.023149283602833748\n",
      "\n",
      "\u001b[1;37mBatch loss 715 : 0.06964796781539917\n",
      "\n",
      "\u001b[1;37mBatch loss 716 : 0.05173056572675705\n",
      "\n",
      "\u001b[1;37mBatch loss 717 : 0.0029770969413220882\n",
      "\n",
      "\u001b[1;37mBatch loss 718 : 0.06472968310117722\n",
      "\n",
      "\u001b[1;37mBatch loss 719 : 0.012430567294359207\n",
      "\n",
      "\u001b[1;37mBatch loss 720 : 0.02057282254099846\n",
      "\n",
      "\u001b[1;37mBatch loss 721 : 0.048361774533987045\n",
      "\n",
      "\u001b[1;37mBatch loss 722 : 0.0470881424844265\n",
      "\n",
      "\u001b[1;37mBatch loss 723 : 0.02248489111661911\n",
      "\n",
      "\u001b[1;37mBatch loss 724 : 0.044933442026376724\n",
      "\n",
      "\u001b[1;37mBatch loss 725 : 0.011493051424622536\n",
      "\n",
      "\u001b[1;37mBatch loss 726 : 0.04441745579242706\n",
      "\n",
      "\u001b[1;37mBatch loss 727 : 0.05730542540550232\n",
      "\n",
      "\u001b[1;37mBatch loss 728 : 0.029377777129411697\n",
      "\n",
      "\u001b[1;37mBatch loss 729 : 0.023719999939203262\n",
      "\n",
      "\u001b[1;37mBatch loss 730 : 0.0203480776399374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;37mBatch loss 731 : 0.012831399217247963\n",
      "\n",
      "\u001b[1;37mBatch loss 732 : 0.0627390518784523\n",
      "\n",
      "\u001b[1;37mBatch loss 733 : 0.04997320473194122\n",
      "\n",
      "\u001b[1;37mBatch loss 734 : 0.013624458573758602\n",
      "\n",
      "\u001b[1;37mBatch loss 735 : 0.03252684697508812\n",
      "\n",
      "\u001b[1;37mBatch loss 736 : 0.01564124785363674\n",
      "\n",
      "\u001b[1;37mBatch loss 737 : 0.023759357631206512\n",
      "\n",
      "\u001b[1;37mBatch loss 738 : 0.02560504525899887\n",
      "\n",
      "\u001b[1;37mBatch loss 739 : 0.03899998962879181\n",
      "\n",
      "\u001b[1;37mBatch loss 740 : 0.027096092700958252\n",
      "\n",
      "\u001b[1;37mBatch loss 741 : 0.03998586907982826\n",
      "\n",
      "\u001b[1;37mBatch loss 742 : 0.02774680405855179\n",
      "\n",
      "\u001b[1;37mBatch loss 743 : 0.020962834358215332\n",
      "\n",
      "\u001b[1;37mBatch loss 744 : 0.05647410824894905\n",
      "\n",
      "\u001b[1;37mBatch loss 745 : 0.03185613080859184\n",
      "\n",
      "\u001b[1;37mBatch loss 746 : 0.03649086132645607\n",
      "\n",
      "\u001b[1;37mBatch loss 747 : 0.034211646765470505\n",
      "\n",
      "\u001b[1;37mBatch loss 748 : 0.035404909402132034\n",
      "\n",
      "\u001b[1;37mBatch loss 749 : 0.040070995688438416\n",
      "\n",
      "\u001b[1;37mBatch loss 750 : 0.042590152472257614\n",
      "\n",
      "\u001b[1;37mBatch loss 751 : 0.09169220179319382\n",
      "\n",
      "\u001b[1;37mBatch loss 752 : 0.027819547802209854\n",
      "\n",
      "\u001b[1;37mBatch loss 753 : 0.029229989275336266\n",
      "\n",
      "\u001b[1;37mBatch loss 754 : 0.02999221533536911\n",
      "\n",
      "\u001b[1;37mBatch loss 755 : 0.0739806741476059\n",
      "\n",
      "\u001b[1;37mBatch loss 756 : 0.020503289997577667\n",
      "\n",
      "\u001b[1;37mBatch loss 757 : 0.017867842689156532\n",
      "\n",
      "\u001b[1;37mBatch loss 758 : 0.03961534798145294\n",
      "\n",
      "\u001b[1;37mBatch loss 759 : 0.024130703881382942\n",
      "\n",
      "\u001b[1;37mBatch loss 760 : 0.027037065476179123\n",
      "\n",
      "\u001b[1;37mBatch loss 761 : 0.02789924666285515\n",
      "\n",
      "\u001b[1;37mBatch loss 762 : 0.02513095550239086\n",
      "\n",
      "\u001b[1;37mBatch loss 763 : 0.03806573525071144\n",
      "\n",
      "\u001b[1;37mBatch loss 764 : 0.03142208233475685\n",
      "\n",
      "\u001b[1;37mBatch loss 765 : 0.04327469319105148\n",
      "\n",
      "\u001b[1;37mBatch loss 766 : 0.019548771902918816\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 1 : 0.11739110201597214\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 2 : 0.06930343061685562\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 3 : 0.09940624237060547\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 4 : 0.11496076732873917\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 5 : 0.03488808125257492\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 6 : 0.057127323001623154\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 7 : 0.034448493272066116\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 8 : 0.05230589956045151\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 9 : 0.05336157977581024\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 10 : 0.03969600796699524\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 11 : 0.04459245502948761\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 12 : 0.06326637417078018\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 13 : 0.040457312017679214\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 14 : 0.042274970561265945\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 15 : 0.06698031723499298\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 16 : 0.03854517638683319\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 17 : 0.05521375685930252\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 18 : 0.01917319931089878\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 19 : 0.05168371647596359\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 20 : 0.03258498013019562\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 21 : 0.032590728253126144\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 22 : 0.12053349614143372\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 23 : 0.022742154076695442\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 24 : 0.02434849739074707\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 25 : 0.049135927110910416\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 26 : 0.0662010908126831\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 27 : 0.030484631657600403\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 28 : 0.051219016313552856\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 29 : 0.036271099001169205\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 30 : 0.01992899551987648\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 31 : 0.052286356687545776\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 32 : 0.03250033035874367\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 33 : 0.06608086824417114\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 34 : 0.022459007799625397\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 35 : 0.018897147849202156\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 36 : 0.018113315105438232\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 37 : 0.06463798880577087\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 38 : 0.05444630607962608\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 39 : 0.04481823742389679\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 40 : 0.0371309332549572\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 41 : 0.06641146540641785\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 42 : 0.048374105244874954\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 43 : 0.047689978033304214\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 44 : 0.050000861287117004\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 45 : 0.08647288382053375\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 46 : 0.02881808578968048\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 47 : 0.006798465736210346\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 48 : 0.05085872486233711\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 49 : 0.07757851481437683\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 50 : 0.1264069527387619\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 51 : 0.05896946042776108\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 52 : 0.08675394207239151\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 53 : 0.037497274577617645\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 54 : 0.02256704308092594\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 55 : 0.046279191970825195\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 56 : 0.0387040451169014\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 57 : 0.0564357228577137\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 58 : 0.0515083372592926\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 59 : 0.030622448772192\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 60 : 0.06378418207168579\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 61 : 0.018107419833540916\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 62 : 0.030061468482017517\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 63 : 0.017730295658111572\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 64 : 0.022629207000136375\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 65 : 0.08942951261997223\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 66 : 0.06669078767299652\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 67 : 0.051539111882448196\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 68 : 0.06113365665078163\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 69 : 0.03713991492986679\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 70 : 0.017899727448821068\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 71 : 0.09155913442373276\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 72 : 0.05616141855716705\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 73 : 0.03330753743648529\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 74 : 0.039950963109731674\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 75 : 0.04263794794678688\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 76 : 0.07448141276836395\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 77 : 0.02592526748776436\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 78 : 0.10136228799819946\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 79 : 0.05639687553048134\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 80 : 0.07059743255376816\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 81 : 0.01216708030551672\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 82 : 0.05105185508728027\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 83 : 0.07015679031610489\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 84 : 0.07994499057531357\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 85 : 0.041550539433956146\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 86 : 0.061957236379384995\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 87 : 0.03161396086215973\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 88 : 0.04388619586825371\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 89 : 0.04096156731247902\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 90 : 0.02025696076452732\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 91 : 0.04966021701693535\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 92 : 0.03916877880692482\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 93 : 0.049101438373327255\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 94 : 0.035813432186841965\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 95 : 0.0436968095600605\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 96 : 0.045582689344882965\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 97 : 0.03498847782611847\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 98 : 0.05757076293230057\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 99 : 0.05079979449510574\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 100 : 0.050439171493053436\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 101 : 0.04573476314544678\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 102 : 0.07257315516471863\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 103 : 0.02238382399082184\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 104 : 0.04387296736240387\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 105 : 0.0574694499373436\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 106 : 0.030778976157307625\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 107 : 0.04885004088282585\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 108 : 0.05747034028172493\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 109 : 0.07519557327032089\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 110 : 0.03635849803686142\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 111 : 0.057217467576265335\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 112 : 0.05444101244211197\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 113 : 0.05652548000216484\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 114 : 0.06149964779615402\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 115 : 0.03456438332796097\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 116 : 0.027895059436559677\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 117 : 0.036421094089746475\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 118 : 0.03097432665526867\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 119 : 0.02593349851667881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;32mValidation Batch loss 120 : 0.07025668770074844\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 121 : 0.04002432897686958\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 122 : 0.0417574942111969\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 123 : 0.02263101376593113\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 124 : 0.02179848589003086\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 125 : 0.0452536903321743\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 126 : 0.027805175632238388\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 127 : 0.04959941282868385\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 128 : 0.043229278177022934\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 129 : 0.10605429857969284\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 130 : 0.06026044860482216\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 131 : 0.044485971331596375\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 132 : 0.058913879096508026\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 133 : 0.10013172775506973\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 134 : 0.031843315809965134\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 135 : 0.0347464382648468\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 136 : 0.031696390360593796\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 137 : 0.03927373141050339\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 138 : 0.06768190115690231\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 139 : 0.022159893065690994\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 140 : 0.006319655571132898\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 141 : 0.005053187720477581\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 142 : 0.027179835364222527\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 143 : 0.028891775757074356\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 144 : 0.00941070169210434\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 145 : 0.019717421382665634\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 146 : 0.049594588577747345\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 147 : 0.04025566950440407\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 148 : 0.034987080842256546\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 149 : 0.032908350229263306\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 150 : 0.025806529447436333\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 151 : 0.012288808822631836\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 152 : 0.04169416427612305\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 153 : 0.023187756538391113\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 154 : 0.03966182842850685\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 155 : 0.03218921273946762\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 156 : 0.028853639960289\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 157 : 0.0272673387080431\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 158 : 0.015278889797627926\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 159 : 0.09434252977371216\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 160 : 0.010044950060546398\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 161 : 0.042419981211423874\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 162 : 0.03910018876194954\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 163 : 0.06716848164796829\n",
      "\n",
      "\u001b[1;32mValidation Batch loss 164 : 0.0605093389749527\n",
      "\n",
      "\u001b[1;33mEpoch 1 :\n",
      "\u001b[1;37mTraining Loss : 0.12676971397780892\n",
      "\u001b[1;32mValidation Loss : 0.04658551313971147\n",
      "\u001b[1;31mDurÃ©e epoch : 582.3525483608246 secondes\n",
      "\n",
      "\u001b[1;32mTest Batch loss 1 : 0.04160366579890251\n",
      "\n",
      "\u001b[1;32mTest Batch loss 2 : 0.023827681317925453\n",
      "\n",
      "\u001b[1;32mTest Batch loss 3 : 0.054785676300525665\n",
      "\n",
      "\u001b[1;32mTest Batch loss 4 : 0.05487435683608055\n",
      "\n",
      "\u001b[1;32mTest Batch loss 5 : 0.041092708706855774\n",
      "\n",
      "\u001b[1;32mTest Batch loss 6 : 0.05462195724248886\n",
      "\n",
      "\u001b[1;32mTest Batch loss 7 : 0.05652996897697449\n",
      "\n",
      "\u001b[1;32mTest Batch loss 8 : 0.03849364444613457\n",
      "\n",
      "\u001b[1;32mTest Batch loss 9 : 0.05638003349304199\n",
      "\n",
      "\u001b[1;32mTest Batch loss 10 : 0.023029932752251625\n",
      "\n",
      "\u001b[1;32mTest Batch loss 11 : 0.06711766868829727\n",
      "\n",
      "\u001b[1;32mTest Batch loss 12 : 0.0408586785197258\n",
      "\n",
      "\u001b[1;32mTest Batch loss 13 : 0.03878117352724075\n",
      "\n",
      "\u001b[1;32mTest Batch loss 14 : 0.025638718158006668\n",
      "\n",
      "\u001b[1;32mTest Batch loss 15 : 0.009066910482943058\n",
      "\n",
      "\u001b[1;32mTest Batch loss 16 : 0.060819827020168304\n",
      "\n",
      "\u001b[1;32mTest Batch loss 17 : 0.05249140411615372\n",
      "\n",
      "\u001b[1;32mTest Batch loss 18 : 0.06152257323265076\n",
      "\n",
      "\u001b[1;32mTest Batch loss 19 : 0.06975796073675156\n",
      "\n",
      "\u001b[1;32mTest Batch loss 20 : 0.06482701003551483\n",
      "\n",
      "\u001b[1;32mTest Batch loss 21 : 0.03026667982339859\n",
      "\n",
      "\u001b[1;32mTest Batch loss 22 : 0.0406198613345623\n",
      "\n",
      "\u001b[1;32mTest Batch loss 23 : 0.07248347997665405\n",
      "\n",
      "\u001b[1;32mTest Batch loss 24 : 0.04001954570412636\n",
      "\n",
      "\u001b[1;32mTest Batch loss 25 : 0.07969655096530914\n",
      "\n",
      "\u001b[1;32mTest Batch loss 26 : 0.08050335198640823\n",
      "\n",
      "\u001b[1;32mTest Batch loss 27 : 0.04952215030789375\n",
      "\n",
      "\u001b[1;32mTest Batch loss 28 : 0.05655026435852051\n",
      "\n",
      "\u001b[1;32mTest Batch loss 29 : 0.04072629287838936\n",
      "\n",
      "\u001b[1;32mTest Batch loss 30 : 0.052281953394412994\n",
      "\n",
      "\u001b[1;32mTest Batch loss 31 : 0.049003664404153824\n",
      "\n",
      "\u001b[1;32mTest Batch loss 32 : 0.059528328478336334\n",
      "\n",
      "\u001b[1;32mTest Batch loss 33 : 0.04302586615085602\n",
      "\n",
      "\u001b[1;32mTest Batch loss 34 : 0.05281717702746391\n",
      "\n",
      "\u001b[1;32mTest Batch loss 35 : 0.021289711818099022\n",
      "\n",
      "\u001b[1;32mTest Batch loss 36 : 0.07153347879648209\n",
      "\n",
      "\u001b[1;32mTest Batch loss 37 : 0.06059812381863594\n",
      "\n",
      "\u001b[1;32mTest Batch loss 38 : 0.08098714798688889\n",
      "\n",
      "\u001b[1;32mTest Batch loss 39 : 0.06597621738910675\n",
      "\n",
      "\u001b[1;32mTest Batch loss 40 : 0.05722702667117119\n",
      "\n",
      "\u001b[1;32mTest Batch loss 41 : 0.05844354256987572\n",
      "\n",
      "\u001b[1;32mTest Batch loss 42 : 0.0706087276339531\n",
      "\n",
      "\u001b[1;32mTest Batch loss 43 : 0.06712475419044495\n",
      "\n",
      "\u001b[1;32mTest Batch loss 44 : 0.05707273632287979\n",
      "\n",
      "\u001b[1;32mTest Batch loss 45 : 0.05488211289048195\n",
      "\n",
      "\u001b[1;32mTest Batch loss 46 : 0.07868345081806183\n",
      "\n",
      "\u001b[1;32mTest Batch loss 47 : 0.04235422611236572\n",
      "\n",
      "\u001b[1;32mTest Batch loss 48 : 0.09020522236824036\n",
      "\n",
      "\u001b[1;32mTest Batch loss 49 : 0.03173001855611801\n",
      "\n",
      "\u001b[1;32mTest Batch loss 50 : 0.06413295120000839\n",
      "\n",
      "\u001b[1;32mTest Batch loss 51 : 0.052404843270778656\n",
      "\n",
      "\u001b[1;32mTest Batch loss 52 : 0.0857834443449974\n",
      "\n",
      "\u001b[1;32mTest Batch loss 53 : 0.07571854442358017\n",
      "\n",
      "\u001b[1;32mTest Batch loss 54 : 0.08330188691616058\n",
      "\n",
      "\u001b[1;32mTest Batch loss 55 : 0.0451275072991848\n",
      "\n",
      "\u001b[1;32mTest Batch loss 56 : 0.09734311699867249\n",
      "\n",
      "\u001b[1;32mTest Batch loss 57 : 0.04130998253822327\n",
      "\n",
      "\u001b[1;32mTest Batch loss 58 : 0.06783543527126312\n",
      "\n",
      "\u001b[1;32mTest Batch loss 59 : 0.03796840086579323\n",
      "\n",
      "\u001b[1;32mTest Batch loss 60 : 0.10833421349525452\n",
      "\n",
      "\u001b[1;32mTest Batch loss 61 : 0.009972513653337955\n",
      "\n",
      "\u001b[1;32mTest Batch loss 62 : 0.028604691848158836\n",
      "\n",
      "\u001b[1;32mTest Batch loss 63 : 0.060712605714797974\n",
      "\n",
      "\u001b[1;32mTest Batch loss 64 : 0.0438176691532135\n",
      "\n",
      "\u001b[1;32mTest Batch loss 65 : 0.017336541786789894\n",
      "\n",
      "\u001b[1;32mTest Batch loss 66 : 0.01283381786197424\n",
      "\n",
      "\u001b[1;32mTest Batch loss 67 : 0.035504408180713654\n",
      "\n",
      "\u001b[1;32mTest Batch loss 68 : 0.05981995910406113\n",
      "\n",
      "\u001b[1;32mTest Batch loss 69 : 0.019306613132357597\n",
      "\n",
      "\u001b[1;32mTest Batch loss 70 : 0.07025453448295593\n",
      "\n",
      "\u001b[1;32mTest Batch loss 71 : 0.030467869713902473\n",
      "\n",
      "\u001b[1;32mTest Batch loss 72 : 0.045076627284288406\n",
      "\n",
      "\u001b[1;32mTest Batch loss 73 : 0.0707051157951355\n",
      "\n",
      "\u001b[1;32mTest Batch loss 74 : 0.06167159229516983\n",
      "\n",
      "\u001b[1;32mTest Batch loss 75 : 0.042074963450431824\n",
      "\n",
      "\u001b[1;32mTest Batch loss 76 : 0.08231089264154434\n",
      "\n",
      "\u001b[1;32mTest Batch loss 77 : 0.011424597352743149\n",
      "\n",
      "\u001b[1;32mTest Batch loss 78 : 0.02675057016313076\n",
      "\n",
      "\u001b[1;32mTest Batch loss 79 : 0.051043782383203506\n",
      "\n",
      "\u001b[1;32mTest Batch loss 80 : 0.10842310637235641\n",
      "\n",
      "\u001b[1;32mTest Batch loss 81 : 0.05127652734518051\n",
      "\n",
      "\u001b[1;32mTest Batch loss 82 : 0.06858978420495987\n",
      "\n",
      "\u001b[1;32mTest Batch loss 83 : 0.05751418694853783\n",
      "\n",
      "\u001b[1;32mTest Batch loss 84 : 0.10354982316493988\n",
      "\n",
      "\u001b[1;32mTest Batch loss 85 : 0.08654788136482239\n",
      "\n",
      "\u001b[1;32mTest Batch loss 86 : 0.07044623047113419\n",
      "\n",
      "\u001b[1;32mTest Batch loss 87 : 0.049318138509988785\n",
      "\n",
      "\u001b[1;32mTest Batch loss 88 : 0.03492969647049904\n",
      "\n",
      "\u001b[1;32mTest Batch loss 89 : 0.046881407499313354\n",
      "\n",
      "\u001b[1;32mTest Batch loss 90 : 0.055142536759376526\n",
      "\n",
      "\u001b[1;32mTest Batch loss 91 : 0.042858801782131195\n",
      "\n",
      "\u001b[1;32mTest Batch loss 92 : 0.04393820837140083\n",
      "\n",
      "\u001b[1;32mTest Batch loss 93 : 0.09180231392383575\n",
      "\n",
      "\u001b[1;32mTest Batch loss 94 : 0.02956114523112774\n",
      "\n",
      "\u001b[1;32mTest Batch loss 95 : 0.03873567283153534\n",
      "\n",
      "\u001b[1;32mTest Batch loss 96 : 0.03256168216466904\n",
      "\n",
      "\u001b[1;32mTest Batch loss 97 : 0.021982621401548386\n",
      "\n",
      "\u001b[1;32mTest Batch loss 98 : 0.027936972677707672\n",
      "\n",
      "\u001b[1;32mTest Batch loss 99 : 0.07737518101930618\n",
      "\n",
      "\u001b[1;32mTest Batch loss 100 : 0.04896192625164986\n",
      "\n",
      "\u001b[1;32mTest Batch loss 101 : 0.05326560512185097\n",
      "\n",
      "\u001b[1;32mTest Batch loss 102 : 0.0482884980738163\n",
      "\n",
      "\u001b[1;32mTest Batch loss 103 : 0.04818933457136154\n",
      "\n",
      "\u001b[1;32mTest Batch loss 104 : 0.049093395471572876\n",
      "\n",
      "\u001b[1;32mTest Batch loss 105 : 0.05760541558265686\n",
      "\n",
      "\u001b[1;32mTest Batch loss 106 : 0.04655743017792702\n",
      "\n",
      "\u001b[1;32mTest Batch loss 107 : 0.03664388507604599\n",
      "\n",
      "\u001b[1;32mTest Batch loss 108 : 0.03791620209813118\n",
      "\n",
      "\u001b[1;32mTest Batch loss 109 : 0.08789276331663132\n",
      "\n",
      "\u001b[1;32mTest Batch loss 110 : 0.06817431002855301\n",
      "\n",
      "\u001b[1;32mTest Batch loss 111 : 0.01589142344892025\n",
      "\n",
      "\u001b[1;32mTest Batch loss 112 : 0.01004849188029766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;32mTest Batch loss 113 : 0.04744454473257065\n",
      "\n",
      "\u001b[1;32mTest Batch loss 114 : 0.015269845724105835\n",
      "\n",
      "\u001b[1;32mTest Batch loss 115 : 0.05768264830112457\n",
      "\n",
      "\u001b[1;32mTest Batch loss 116 : 0.07395852357149124\n",
      "\n",
      "\u001b[1;32mTest Batch loss 117 : 0.08158751577138901\n",
      "\n",
      "\u001b[1;32mTest Batch loss 118 : 0.05849819630384445\n",
      "\n",
      "\u001b[1;32mTest Batch loss 119 : 0.08079972118139267\n",
      "\n",
      "\u001b[1;32mTest Batch loss 120 : 0.03491669520735741\n",
      "\n",
      "\u001b[1;32mTest Batch loss 121 : 0.072422094643116\n",
      "\n",
      "\u001b[1;32mTest Batch loss 122 : 0.07711680233478546\n",
      "\n",
      "\u001b[1;32mTest Batch loss 123 : 0.040944747626781464\n",
      "\n",
      "\u001b[1;32mTest Batch loss 124 : 0.05172586068511009\n",
      "\n",
      "\u001b[1;32mTest Batch loss 125 : 0.053630877286195755\n",
      "\n",
      "\u001b[1;32mTest Batch loss 126 : 0.10420077294111252\n",
      "\n",
      "\u001b[1;32mTest Batch loss 127 : 0.019975222647190094\n",
      "\n",
      "\u001b[1;32mTest Batch loss 128 : 0.08123281598091125\n",
      "\n",
      "\u001b[1;32mTest Batch loss 129 : 0.07940315455198288\n",
      "\n",
      "\u001b[1;32mTest Batch loss 130 : 0.018268810585141182\n",
      "\n",
      "\u001b[1;32mTest Batch loss 131 : 0.0802968442440033\n",
      "\n",
      "\u001b[1;32mTest Batch loss 132 : 0.0399785190820694\n",
      "\n",
      "\u001b[1;32mTest Batch loss 133 : 0.07278142869472504\n",
      "\n",
      "\u001b[1;32mTest Batch loss 134 : 0.07806137204170227\n",
      "\n",
      "\u001b[1;32mTest Batch loss 135 : 0.051449403166770935\n",
      "\n",
      "\u001b[1;32mTest Batch loss 136 : 0.029008347541093826\n",
      "\n",
      "\u001b[1;32mTest Batch loss 137 : 0.05951504409313202\n",
      "\n",
      "\u001b[1;32mTest Batch loss 138 : 0.039063695818185806\n",
      "\n",
      "\u001b[1;32mTest Batch loss 139 : 0.015180269256234169\n",
      "\n",
      "\u001b[1;32mTest Batch loss 140 : 0.07557865977287292\n",
      "\n",
      "\u001b[1;32mTest Batch loss 141 : 0.0937148854136467\n",
      "\n",
      "\u001b[1;32mTest Batch loss 142 : 0.05723602697253227\n",
      "\n",
      "\u001b[1;32mTest Batch loss 143 : 0.026643969118595123\n",
      "\n",
      "\u001b[1;32mTest Batch loss 144 : 0.0721917450428009\n",
      "\n",
      "\u001b[1;32mTest Batch loss 145 : 0.06611688435077667\n",
      "\n",
      "\u001b[1;32mTest Batch loss 146 : 0.021412242203950882\n",
      "\n",
      "\u001b[1;32mTest Batch loss 147 : 0.03632634878158569\n",
      "\n",
      "\u001b[1;32mTest Batch loss 148 : 0.05785852298140526\n",
      "\n",
      "\u001b[1;32mTest Batch loss 149 : 0.07316618412733078\n",
      "\n",
      "\u001b[1;32mTest Batch loss 150 : 0.027520393952727318\n",
      "\n",
      "\u001b[1;32mTest Batch loss 151 : 0.0604390911757946\n",
      "\n",
      "\u001b[1;32mTest Batch loss 152 : 0.05768971890211105\n",
      "\n",
      "\u001b[1;32mTest Batch loss 153 : 0.04983006790280342\n",
      "\n",
      "\u001b[1;32mTest Batch loss 154 : 0.04287058860063553\n",
      "\n",
      "\u001b[1;32mTest Batch loss 155 : 0.04291268438100815\n",
      "\n",
      "\u001b[1;32mTest Batch loss 156 : 0.04941903054714203\n",
      "\n",
      "\u001b[1;32mTest Batch loss 157 : 0.05262617766857147\n",
      "\n",
      "\u001b[1;32mTest Batch loss 158 : 0.04661338031291962\n",
      "\n",
      "\u001b[1;32mTest Batch loss 159 : 0.07323300093412399\n",
      "\n",
      "\u001b[1;32mTest Batch loss 160 : 0.03311185538768768\n",
      "\n",
      "\u001b[1;32mTest Batch loss 161 : 0.10451953858137131\n",
      "\n",
      "\u001b[1;32mTest Batch loss 162 : 0.06482987105846405\n",
      "\n",
      "\u001b[1;32mTest Batch loss 163 : 0.033731609582901\n",
      "\n",
      "\u001b[1;32mTest Batch loss 164 : 0.07069573551416397\n",
      "\n",
      "Test Loss : 0.053557533503896214\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    EPOCHS = 1\n",
    "    LR = 1e-4\n",
    "    \n",
    "    optimizer = torch.optim.Adam(modelBERT_FT.parameters(), lr=LR, eps=5e-8)\n",
    "    module = train_loop(module=modelBERT_FT,\n",
    "                        EPOCHS=EPOCHS, \n",
    "                        train_dataset=train_dataloaded, \n",
    "                        val_dataset=val_dataloaded,\n",
    "                        optimizer=optimizer)\n",
    "    device = 'cpu'\n",
    "    predictions, true_targets = evaluate(module, \n",
    "                                         test_dataloaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'active.all.allocated': 1087630, 'active.all.current': 406, 'active.all.freed': 1087224, 'active.all.peak': 1024, 'active.large_pool.allocated': 506367, 'active.large_pool.current': 152, 'active.large_pool.freed': 506215, 'active.large_pool.peak': 381, 'active.small_pool.allocated': 581263, 'active.small_pool.current': 254, 'active.small_pool.freed': 581009, 'active.small_pool.peak': 721, 'active_bytes.all.allocated': 4169757224960, 'active_bytes.all.current': 894567424, 'active_bytes.all.freed': 4168862657536, 'active_bytes.all.peak': 3127888384, 'active_bytes.large_pool.allocated': 4083723288064, 'active_bytes.large_pool.current': 893321216, 'active_bytes.large_pool.freed': 4082829966848, 'active_bytes.large_pool.peak': 3116457984, 'active_bytes.small_pool.allocated': 86033936896, 'active_bytes.small_pool.current': 1246208, 'active_bytes.small_pool.freed': 86032690688, 'active_bytes.small_pool.peak': 13091328, 'allocated_bytes.all.allocated': 4169757224960, 'allocated_bytes.all.current': 894567424, 'allocated_bytes.all.freed': 4168862657536, 'allocated_bytes.all.peak': 3127888384, 'allocated_bytes.large_pool.allocated': 4083723288064, 'allocated_bytes.large_pool.current': 893321216, 'allocated_bytes.large_pool.freed': 4082829966848, 'allocated_bytes.large_pool.peak': 3116457984, 'allocated_bytes.small_pool.allocated': 86033936896, 'allocated_bytes.small_pool.current': 1246208, 'allocated_bytes.small_pool.freed': 86032690688, 'allocated_bytes.small_pool.peak': 13091328, 'allocation.all.allocated': 1087630, 'allocation.all.current': 406, 'allocation.all.freed': 1087224, 'allocation.all.peak': 1024, 'allocation.large_pool.allocated': 506367, 'allocation.large_pool.current': 152, 'allocation.large_pool.freed': 506215, 'allocation.large_pool.peak': 381, 'allocation.small_pool.allocated': 581263, 'allocation.small_pool.current': 254, 'allocation.small_pool.freed': 581009, 'allocation.small_pool.peak': 721, 'inactive_split.all.allocated': 458484, 'inactive_split.all.current': 48, 'inactive_split.all.freed': 458436, 'inactive_split.all.peak': 112, 'inactive_split.large_pool.allocated': 295309, 'inactive_split.large_pool.current': 42, 'inactive_split.large_pool.freed': 295267, 'inactive_split.large_pool.peak': 79, 'inactive_split.small_pool.allocated': 163175, 'inactive_split.small_pool.current': 6, 'inactive_split.small_pool.freed': 163169, 'inactive_split.small_pool.peak': 35, 'inactive_split_bytes.all.allocated': 3811023832064, 'inactive_split_bytes.all.current': 237894656, 'inactive_split_bytes.all.freed': 3810785937408, 'inactive_split_bytes.all.peak': 461697024, 'inactive_split_bytes.large_pool.allocated': 3722082121216, 'inactive_split_bytes.large_pool.current': 234946560, 'inactive_split_bytes.large_pool.freed': 3721847174656, 'inactive_split_bytes.large_pool.peak': 455366144, 'inactive_split_bytes.small_pool.allocated': 88941710848, 'inactive_split_bytes.small_pool.current': 2948096, 'inactive_split_bytes.small_pool.freed': 88938762752, 'inactive_split_bytes.small_pool.peak': 6462464, 'max_split_size': -1, 'num_alloc_retries': 0, 'num_device_alloc': 97, 'num_device_free': 0, 'num_ooms': 0, 'num_sync_all_streams': 0, 'oversize_allocations.allocated': 0, 'oversize_allocations.current': 0, 'oversize_allocations.freed': 0, 'oversize_allocations.peak': 0, 'oversize_segments.allocated': 0, 'oversize_segments.current': 0, 'oversize_segments.freed': 0, 'oversize_segments.peak': 0, 'requested_bytes.all.allocated': 4151305155882, 'requested_bytes.all.current': 893153744, 'requested_bytes.all.freed': 4150412002138, 'requested_bytes.all.peak': 3121134788, 'requested_bytes.large_pool.allocated': 4065393846272, 'requested_bytes.large_pool.current': 891908096, 'requested_bytes.large_pool.freed': 4064501938176, 'requested_bytes.large_pool.peak': 3109720064, 'requested_bytes.small_pool.allocated': 85911309610, 'requested_bytes.small_pool.current': 1245648, 'requested_bytes.small_pool.freed': 85910063962, 'requested_bytes.small_pool.peak': 13077692, 'reserved_bytes.all.allocated': 3384803328, 'reserved_bytes.all.current': 3384803328, 'reserved_bytes.all.freed': 0, 'reserved_bytes.all.peak': 3384803328, 'reserved_bytes.large_pool.allocated': 3370123264, 'reserved_bytes.large_pool.current': 3370123264, 'reserved_bytes.large_pool.freed': 0, 'reserved_bytes.large_pool.peak': 3370123264, 'reserved_bytes.small_pool.allocated': 14680064, 'reserved_bytes.small_pool.current': 14680064, 'reserved_bytes.small_pool.freed': 0, 'reserved_bytes.small_pool.peak': 14680064, 'segment.all.allocated': 97, 'segment.all.current': 97, 'segment.all.freed': 0, 'segment.all.peak': 97, 'segment.large_pool.allocated': 90, 'segment.large_pool.current': 90, 'segment.large_pool.freed': 0, 'segment.large_pool.peak': 90, 'segment.small_pool.allocated': 7, 'segment.small_pool.current': 7, 'segment.small_pool.freed': 0, 'segment.small_pool.peak': 7})\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29674)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids.max())\n",
    "print(inputs.input_ids.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saves/tokenizer/BERT_FT\\\\tokenizer_config.json',\n",
       " './saves/tokenizer/BERT_FT\\\\special_tokens_map.json',\n",
       " './saves/tokenizer/BERT_FT\\\\vocab.txt',\n",
       " './saves/tokenizer/BERT_FT\\\\added_tokens.json',\n",
       " './saves/tokenizer/BERT_FT\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enregistrement du modÃ¨le\n",
    "\n",
    "modelBERT_FT.save_pretrained('./saves/model/BERT_FT')\n",
    "tokenizerBERT_FT.save_pretrained('./saves/tokenizer/BERT_FT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pfr)",
   "language": "python",
   "name": "pfr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
